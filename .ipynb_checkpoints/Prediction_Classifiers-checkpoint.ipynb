{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the pre and post test data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the '|' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, encoding = 'mbcs')\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['KIScore'] = df['KIScore'].astype(int)\n",
    "    df['Answer'] = df['Answer'].astype(str)\n",
    "    # Filters if needed later on\n",
    "    #filtered_data = df[\"Answer\"].notnull()\n",
    "    #filtered_data = df[df[\"KIScore\"] != 1 & df['Answer'].notnull() & df[\"KIScore\"].notnull()]\n",
    "    #df_narrative = df[filtered_data]\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda - Steve \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    #print the report on category breakdown, might need these counts later\n",
    "    #print(\"Creating training data... category breakdown:\")\n",
    "    #sorted_product_counts = df_narrative.Category.value_counts(ascending=True)\n",
    "    #print(sorted_product_counts)\n",
    "    #sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Categories\");\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into training and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set - Currently only genereates dev and test data\n",
    "#Modify the function later to keep some data as test data as well\n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    #randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['WISEID', 'Answer', 'KIScore']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    #separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    #get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"GHG2/GHG2.csv\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checker created by Peter Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    #return words\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return word\n",
    "    #return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = lambda x : ' '.join(i for i in list(map(correct, tokens_target(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_set['Answer'] = train_set['Answer'].apply(spell_checker)\n",
    "train_set['Answer'] = train_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118459.0</td>\n",
       "      <td>picture is simalar green keeps atmasphere sola...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139823.0</td>\n",
       "      <td>contain heat energy allow rays light</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150246.0</td>\n",
       "      <td>i think picture shows projects plants picture ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136504.0</td>\n",
       "      <td>i think is solar oven th grade did solar had p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118591.0</td>\n",
       "      <td>yes are glass greenhouse is reflecting sun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  118459.0  picture is simalar green keeps atmasphere sola...        2\n",
       "1  139823.0               contain heat energy allow rays light        4\n",
       "2  150246.0  i think picture shows projects plants picture ...        3\n",
       "3  136504.0  i think is solar oven th grade did solar had p...        2\n",
       "4  118591.0         yes are glass greenhouse is reflecting sun        2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dev_set['Answer'] = dev_set['Answer'].apply(spell_checker)\n",
    "dev_set['Answer'] = dev_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136414.0</td>\n",
       "      <td>i said picture is bringing light cook somethin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149960.0</td>\n",
       "      <td>act same solar oven let air don t let</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154206.0</td>\n",
       "      <td>solar oven use plastic wrap act same atmospher...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150089.0</td>\n",
       "      <td>picture is similar has glass same solar oven m...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150165.0</td>\n",
       "      <td>are don t have conduction convection pictures ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  136414.0  i said picture is bringing light cook somethin...        3\n",
       "1  149960.0              act same solar oven let air don t let        4\n",
       "2  154206.0  solar oven use plastic wrap act same atmospher...        3\n",
       "3  150089.0  picture is similar has glass same solar oven m...        2\n",
       "4  150165.0  are don t have conduction convection pictures ...        3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies that need to be attempted </b>\n",
    "1. Feature Selection attempted:\n",
    "    1. counts of Unigrams only \n",
    "    2. ... Unigrams and Bigrams\n",
    "    3. ... Unigrams, Bigrams, and Trigrams\n",
    "    4. ... Bigrams and Trigrams\n",
    "    5. ... 4- and 5-gram combinations\n",
    "    6. The use of TF-IDF, with IDF and without\n",
    "    7. Word tokens that included punctuation and numbers\n",
    "    8. Word tokens with letters only, filtering punctuation or splitting on punctuation\n",
    "    9. Lemmatizing using Word Net\n",
    "    10. Stemming using Snowball\n",
    "    11. With and without stopwords\n",
    "    12. With and without lowercasing\n",
    "    13. Chunking out all words that are not nouns.\n",
    "    14. Stemming user Porter and Lancaster stemmers.\n",
    "    15. Checking most common hypernyms of nouns in the review to categorise reviews better.\n",
    "    16. Using feature unions in pipelines to select specific features.\n",
    "2. Classifiers used:\n",
    "    1. Linear: Naive Bayes, Linear Regression, Stochastic Gradiant Descent\n",
    "    2. SVC and Linear SVC (One vs One, One vs Many)\n",
    "    3. K - Nearest Neighbor\n",
    "    4. MLP\n",
    "    5. Voting classifiers with hard and soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Answer\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Answer\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple Naive Bayes classifier for Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB_model(train_set, arr_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train, arr_dev = transform_dfs_to_arrays(train_set, dev_set)\n",
    "nb_model = train_NB_model(train_set, arr_train)\n",
    "nb_predictions = nb_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52356020942408377"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNearest_model(train_set, arr_train):\n",
    "    #Should add and experiement with more parameters and algorithms for nearest neighbor\n",
    "    nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54308093994778073"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_model = train_KNearest_model(train_set, arr_train)\n",
    "ne_predictions = neigh_model.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, ne_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad for a start, lets move onto Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LR_model(train_set, arr_train):\n",
    "    logreg = LogisticRegression()\n",
    "    lr_model = logreg.fit(arr_train, train_set.KIScore)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = train_LR_model(train_set, arr_train)\n",
    "lr_predictions = lr_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55759162303664922"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already nearing 80s!!!! But remember, need to measure Cohen's Kappa, not percetage correct. Also start plotting confusion matrix and extract errors once the classifiers are worked out.\n",
    "\n",
    "Lets start with the pipeline for the best features and get to feature detections using SVM. Also need to perform all the combinations mentioned before (Including preprocessing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    #this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline attempts - Best features will be decided using Grid Search. Lets just setup a baseline for now.\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#Note: add probability True to SVC classifier to be able to use predict probability function, which\n",
    "# is crucial for the the ensemble methods tried later\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\")),\n",
    "                      ('tfidf', TfidfTransformer(use_idf = True, norm='l2')),\n",
    "                      ('log', LogisticRegression(class_weight = None )),\n",
    "                      ('clf', SVC(C = 1000000.0, gamma='auto', kernel='linear', probability = True))])\n",
    "                      #('clf', LinearSVC(C=1.0, random_state=69, penalty='l2', dual=True, tol=1e-5, class_weight = None))])\n",
    "                      #('clf', OneVsOneClassifier(LinearSVC(random_state=0)))])                    \n",
    "                      #('clf', SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48691099476439792"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predictor = text_clf.fit(train_set[\"Answer\"], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = pipeline_predictor.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>120</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>57</td>\n",
       "      <td>171</td>\n",
       "      <td>110</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1    2    3   4   5  All\n",
       "True                                \n",
       "1          34    3    0   0   0   37\n",
       "2          20  120   24   6   0  170\n",
       "3           3   37   67  12   1  120\n",
       "4           0    9   17  15   4   45\n",
       "5           0    2    2   1   6   11\n",
       "All        57  171  110  34  11  383"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some ensemble classifiers (Both averaging and boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53926701570680624"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f_clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C = 100, penalty=\"l1\", dual = False))),\n",
    "  ('classification', forest_clf)\n",
    "])\n",
    "\n",
    "forest_predictor = f_clf.fit(arr_train, train_set.KIScore)\n",
    "f_predicted = forest_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, f_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40314136125654448"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "ada_predictor = ada_clf.fit(arr_train, train_set.KIScore)\n",
    "a_predicted = ada_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, a_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50785340314136129"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0).fit(arr_train, train_set.KIScore)\n",
    "grad_predictor = grad_clf.fit(arr_train, train_set.KIScore)\n",
    "grad_predicted = grad_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, grad_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And moving on to voting classifier with hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54450261780104714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf5', clf5), ('clf8', clf8),\n",
    "                                   ('clf9', clf9)], voting='hard')\n",
    "\n",
    "eclf_predictor = eclf.fit(arr_train, train_set.KIScore)\n",
    "v_predicted = eclf_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, v_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SGD', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Voting classifier with soft voting\n",
    "\n",
    "Note: The MLP classifier improved accuracy for both hard and sofr voting\n",
    "\n",
    "But, I need to do a ton of cross validation for the correct parameters and classifiers for each question type. Not to mention, need to get the grid search working well for these things.\n",
    "\n",
    "TODO: Find optimal weights for the classifiers\n",
    "\n",
    "TODO: Need to do feature engineering to get better parameters. This isnt working too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_s = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], voting='soft')\n",
    "\n",
    "eclf_s_predictor = eclf_s.fit(arr_train, train_set.KIScore)\n",
    "s_predicted = eclf_s_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, s_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a brute force method to find the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w4', 'w5', 'w6', 'mean', 'std'))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "t0 = time()\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w4 in range(1,4):\n",
    "            for w5 in range(1,4):\n",
    "                for w6 in range(1,4):\n",
    "                        if len(set((w1,w2,w4,w5,w6))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        t0 = time()\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[w1, w2, w4, w5, w6], voting = 'soft')\n",
    "                        scores = cross_val_score(eclf, \n",
    "                                                 arr_train,\n",
    "                                                 train_set.KIScore,\n",
    "                                                 cv=3,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs= -1)\n",
    "                        \n",
    "                        print(\"done in %0.3fs\" % (time() - t0))\n",
    "                        df.loc[i] = [w1, w2, w4, w5, w6, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "                        \n",
    "#print(\"done in %0.3fs\" % (time() - t0))\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54712041884816753"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 3, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "eclf_w_predictor = eclf_w.fit(arr_train, train_set.KIScore)\n",
    "w_predicted = eclf_w_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, w_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! Not very good\n",
    "\n",
    "Lets see which categories we are getting wrong (Don't be 2!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(dev_set.KIScore, w_predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it looks like we should try and get more data. Might not be possible without mixing up student responses.\n",
    "\n",
    "To squeeze voting classifiers into the grid, I'm restricted to using only classifiers that provide a predict_pobability function. Might be worth trying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_grid = grid_search.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have exhausted all classifiers without really messing around with feature engineering or feature selection, lets add custom features to the pipeline using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Classifier for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72845953002610964"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "# Weights - 3 3 1 2 1\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),   \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7389033942558747"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Light(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('light' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'white' in [ps.stem(i) for i in text.split()])\n",
    "                and 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]    \n",
    "    \n",
    "class Keywords_Albedo(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Albedo': 'albedo' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Trap': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_light', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Light()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('key_words_albedo', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Albedo()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline2.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classifier for Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55235602094240843"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]\n",
    "                or 'sun' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Reflect(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Reflect': 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_reflect', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Reflect()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    ('feature_selection', SelectPercentile(chi2, percentile=30)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline3.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58115183246073299"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Word_All(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'All': 'all' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'both' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_all', Pipeline([ # Give low weight\n",
    "                    ('All', Word_All()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),\n",
    "    ('feature_selection', SelectPercentile(chi2, percentile=30)),\n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 30   3   0   0   0]\n",
      " [  0 109  16   3   0]\n",
      " [  0  33  65  17   0]\n",
      " [  0  11  47  16   2]\n",
      " [  0   0   8  20   2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEpCAYAAAD4Vxu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n50LygumccyF5wXTOuZD+P5mfVwnycFyWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2748a9c3a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(dev_set.KIScore, predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act same solar oven let air don t let\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "solar oven use plastic wrap act same atmosphere glass pictures include conduction radiation ad convection solar oven\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "are don t have conduction convection pictures have radiation sun is beating\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "picture traps heat oven do picture b is showing greenhouse effect sunlight warms earth is similar oven releases heat warm something\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "anserwser is traps ray sin makes infrared turn heat cells\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar trap heat radiation trap heat\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "pictures b are similar solar oven solar oven traps heat use good insulator material reason is sunlight hits solar bounces escapes heat is trapped sun rays\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "is similar plastic rap keeps heat box is similar tin foil reflects heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "yes look similar earth s is glass green house sun reflects atmosphere sun reflects green house glass\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "is trapped space heat be solar oven suns heat is helping greenhouse work oven\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "pictures have ways trap heat way are solar oven are being cooked alive\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "purpose greenhouse is retain heat allows heat enter allows minimal amount heat leave room solar oven does picture b is similar balances heat more greenhouse does\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "are similar give heat keep everything warm greenhouse sun traps heat solar oven helps heat oven sun helps heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "reason is green house absorb heat heat sun wouldn t leave green house second reason is cause radiation green house has sunlight plants grow\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture b is similar i saw energy leave box computer program picture shows energy leaves box picture shows energy leaves earth\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "pictures go barrier reaches plant earth create heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "retrieve light sun are solar heat solar oven uses heat electricity\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "is similar glass make heat go get hot\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "picture is similar radiation goes glass blue lines reflect other solar oven\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "inside atmosphere greenhouse is hot inferred radiation gets trapped glass atmosphere glass everything heats things are made hold heat trap\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "are similar earth collects suns energy does green house solar oven\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "sran wrap is atmosphere windoe foil is surface earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "reason is solar oven heats inside box pictures b\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "reason re is have opening inside reason re similar is pictures have same sunlight purpose\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture b is solar oven sun goes doesnt get\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "b are similar solar oven heat is trapped atmosphere glass heat is caused light sun is beaming atmosphere hits earth plants\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "are similar solar oven keep heat escaping\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "reason is use plastic wrap atmosphere second reason is realises heat traps\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture b has sorta cover sun acts light picture has glass panels cover sun shines goes\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "part b is similar sun is reflecting going earth sun is heating earth sun is heating solar ovens\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "picture is similar greenhouse is giving heat s picture b is similar is holding heat reflecting heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "greenhouse is bigger version solar cooking supposed keep plants warm winter atmosphere is larger version greenhouse solar oven captures heat greenhouse solar oven\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar pictures attract heat trap teh heat\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "show heat going staying coming\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "solar oven is effective is designed greenhouse solar ovens utilize greenhouse effect solar radiation heats gas\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "solar oven allows heat stay uses radiation sun power uses heat sun create\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture b is similar solar oven shows solar oven does is sun rays deflecting solar oven turning heat go solar oven atmosphere earth are solar ovens atmosphere earth is inside box collecting heat sunlight\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "i m familiar solar oven i know is box has reflective metal panels top seems solar oven absorb heat sun s light rays reflect energy seems pretty similar picture b\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar oven something is cooking heat is bouncing plants earth are objects are heating radiation gets trapped bounces\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "uses greenhouse example whitch is solar oven does let sun light escape\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "is same keep heat soler oven\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "solar oven uses solar light keep heat oven order cook greenhouse earth keep heat use solar energy\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "trap heat own way have heat area\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "b is similar heat comes solar energy bounce stay second b is similar atmosphere acts big piece glass other material\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "trap light heat escaping makes warmer earth greenhouse solar oven\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "solar oven traps heat pictures are powered sun\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "first one is shows radiation fact glass keeps\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "plastic wrap acts greenhouse oven keep gases\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "are preventing heat leaving increasing temperature\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "are similar are using solar power have something is transferring light heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are trapping heat using greenhouse effect heat interior require solar radiation other source energy\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "yes are similar solar oven trap heat sun is solar oven does\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "im guessing ovens heat ovean cook green house is traping heat other is same\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "have reflection items atmosphere glass plastic wrap use conduction convection radiation heat whats\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "are similar use sun s natural energy heat things barrier\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture b is similar solar oven earth is using heat energy keep solar oven uses eat energy make something see works second reason is is heating earth solar oven\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "yes b are same react same way solar oven\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "have greenhouse effect way capturing radiation order convert heat heat is inside surface\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "picture is similar solar oven need silver stuff top need sun energy\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "is bad\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "is similar picture picture shows house heat energy\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "greenhouse earth have radiation conduction convection have something keeps radiation\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "reason is greenhouse uses sun keep plants growing reason is greenhouse doesn t get hot\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "i think are similar solar oven i think solar oven is oven is powered sun\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "reason is keep energy let energy pass\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "goal solar ovens was trap heat possible see build temperature design flow light energy is were aiming\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "yes are similar solar energy helps plants grow green house solar oven cooked food solar energy\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture shows green house purpose is attract more heat solar sun s light waves are able penetrate glass are able escape is necessary solar oven order heat food\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "are similer solor keeps heat pictures\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "absorb sunlight get trap sunlight stays is\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are solar oven heat using energy sun contain parts closed container clear roof etc\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "solar glass is keeping heat picture b\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "reason are similar solar oven created is solar oven pictures b reflect light absorbs light\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "first reason is take light second reason is has glass panel b has something similar\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "is similar traps heat heat inside paper absorbs heat causing heat more\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "are similar trap heat like plexi glass catch heat keep\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "reason are similar is are trapping heat solar oven reason are similar is first picture is radiation trap heat space second picture traps heat lets heat escape\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "keep inside warm absorb radiation\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "question energy wont be able escape question energy is going energy is ascapeing\n",
      "Graded as: 4\n",
      "Actual grade is 2\n",
      "\n",
      "are same show rays heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "are similar have sunlight panels help plants\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "pictures is similar solar oven energy power solar oven comes sun heat food picture b is similar solar oven solar oven absorbs heat energy sun uses\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "pictures b sunlight is passing atmosphere glass pass plexiglas solar oven sunlight is bouncing earths surface plants is getting absorbed black paper solar oven\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are shows energy goes thing does something\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar trap heat solar oven traps heat atmosphere greenhouses trap heat inside reason is use sun warmth\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "reason is let light shine make hot do samething\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "reason pictures are similar is solar oven model showed solar radiation came box turned heat infrared radiation similar air is different inside greenhouse atmosphere outside reason pictures are similar is solar radiation rays get stuck box greenhouse cannot get\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "first reason is traps suns heat solar oven second reason is suns rays are trapped are used plants\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "is similar heat is being bounced plants b is simular atmosphere is bouncing heat earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "are similar earth collects suns energy does green house solar oven\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i think solar oven brings heat sun s rays are same example\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "bouncing place doing exact same thing\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "are similar solar oven let light keep heat get heated\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "picture is similar s holding heat reflects plants\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "pictures show solar radiation entering system is solar oven receives pictures show infrared radiation being reflected certain material cover is solar does keeps heat oven\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "looks heating plants solar heat sunlight\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "solar radiation is entering atmosphere greenhouse insulator is entering solar ovens glass plastic solar oven has insulator keeps heat do pictures glass atmosphere are insulators retain heat\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "are similar solar oven show radiation conduction convection\n",
      "Graded as: 4\n",
      "Actual grade is 2\n",
      "\n",
      "get hot air gets hot air produces air\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "are similar have something heat such food have something such atmosphere keeping heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "captures solar energy turns heat energy releases infrared radiation relies sun do things sun t capture solar energy turn heat energy\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "reflects heat solar oven contains heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "itll attract sun keep heat\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "solar oven absorbed heat enclosed keep high temperature greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "are similar glass atmosphere solar oven shown pictures are similar glass atmosphere reflects box shown pictures\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "picture b is similar solar heat gets trapped open heat goes picture b looks sun s heat goes earth gets released\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "have similar qualities require heat sun heats is had conduction convection radiation\n",
      "Graded as: 4\n",
      "Actual grade is 2\n",
      "\n",
      "greenhouse room atmosphere solar oven takes solar radiation order increase temperature interior solar oven has plastic wrap acts glass greenhouse room atmosphere earth trap solar radiation keeps heat\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "are simaler are reflecting heat is everyything\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "s similar solar radiation goes oven bounce reason is are close source heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar trap heat get hot\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "are similar solar oven glass atmosphere plexi glass let light heat green house earth solar oven are same heat intirior are same work same way use solar energy heat slighty reflect keep heat energy trapped\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "are similar is conduction convection happening\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "are similar retain solar oven radiaton goes turns inito heat\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "reason are similar is solar oven does same thing greenhouse atmosphere lets heat second reason are similar solar oven is heat\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "are simalar show greenhouse effect absorbing heat reflecting heat shows solar radiation passing glass panels atmoshphere keeping heat escaping\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "inside picture is showing heat go going same solar oven\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "models get heat be trapped stop heat escaping making inside hotter\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar solar oven uses sun s rays heat traps heat trapped warms stuff\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "b is similar heat energy reflects absorb heat heat makes is absorbed hot\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "response question is pictures b are similar solar oven hot air enters glass atmosphere stay much air is let are similar air pass glass atmosphere plastic wrap heats inside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "pictures are similar objects are trying absorb radiation\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are using radiation heat something have similar things bouncing\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture b is similar solar oven are arrows come sun bounce earth s surface dotted line circling earth is radiation going dotted line\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "looks box shape shows trapps\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "sun ray get warm warm wont go sun ray go go\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "i think solar heat sun be allowed oven get trapped item oven get constant heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "yes are attracted sun heat bounces travels\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "keep energy use sun energy\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "atmosphere glass greenhouse clear covering solar let heat something heat bounce\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are bringing sun rays using light do something example picture is using sunlight growing plants picture b is using bring heat earth solar oven is using cook things\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture b are similar solar oven heat reflect oven get trapped solar oven is similar picure b are using solar use materials make heat reflect oven heat reflects earth green house stays earth greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar warm heat things are\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "heat reflects cold air goes hot air goes\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "is right\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "i said are similar shows pictures sun rays going heating atmosphere coming affecting temperature\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "yes are similar earth takes heat energy green house takes heat energy\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "have heat source something keep heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "are similar trap heat energy warm objects\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "have way take energy way keep heat\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "act same way function solar oven\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "picture b shows heat is going earth trying come gets trapped atmosphere picture shows heat is going glass going room occur solar oven\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "pictures are similar solar oven something covering top have something covering top lets heat atmosphere glass cover top lets heat have rays heat bounces trying escape\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "pictures retain heat inside area\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "picture is similar greenhouse is giving heat s greenhouse picture b is similar atmosphere is holding heat is giving heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "sun makes ight heats things green house is making warm giving plants light stay warm\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "hskbytsnzgkuvkcjrwqetk f e pol\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "pictures b are same sun is going earth house\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i chose yes are similar picture see light is going greenhouse is same thing picture b\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "are similar earth is house other picture\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "are slimmer trap heat reflect heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "have sun light room keeps heat room plants\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i chose answer try attract heat rise temperature absorb trying lose least amount heat possible\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "yes are similar trap heat store energy\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "i think are similar greenhouse sun is reflecting glass make warm b is similar atmosphere is acting glass sun s reflection is bouncing heat is going makes earth warm solar oven\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "are similar is showing project other is showing sun come earth oven be\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "solar ovens need sunlight produce energy picture b is doing heat escapes picture heat doesn t escape\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "solar ovens work same way solar greenhouse reflects light\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "reflection is foil black paper holds heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "trap heat inside are powered sr\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[2]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[1]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[2])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prediction probabilities to see confidence level of predictions and defer to manual grader if needed. Work with the prediction probability feature of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56343279607120411"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "               ('eclf2', pipeline2),\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'eclf2__transformer_weights_body_stats': (0.5, 0.75, 1.0),\n",
    "    'eclf2__transformer_key_words_dark': (0.5, 0.75, 1.0)\n",
    "}\n",
    "\n",
    "#parameters = {}\n",
    "\n",
    "grid_search = GridSearchCV( pipe3, parameters, n_jobs=-1, verbose=1, cv = 3)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe3.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_set[\"Answer\"], train_set.KIScore)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
