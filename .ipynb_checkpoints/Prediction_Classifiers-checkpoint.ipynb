{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the pre and post test data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the '|' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, encoding = 'mbcs')\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['KIScore'] = df['KIScore'].astype(int)\n",
    "    df['Answer'] = df['Answer'].astype(str)\n",
    "    # Filters if needed later on\n",
    "    #filtered_data = df[\"Answer\"].notnull()\n",
    "    #filtered_data = df[df[\"KIScore\"] != 1 & df['Answer'].notnull() & df[\"KIScore\"].notnull()]\n",
    "    #df_narrative = df[filtered_data]\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda - Steve \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    #print the report on category breakdown, might need these counts later\n",
    "    #print(\"Creating training data... category breakdown:\")\n",
    "    #sorted_product_counts = df_narrative.Category.value_counts(ascending=True)\n",
    "    #print(sorted_product_counts)\n",
    "    #sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Categories\");\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into training and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set - Currently only genereates dev and test data\n",
    "#Modify the function later to keep some data as test data as well\n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    #randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['WISEID', 'Answer', 'KIScore']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    #separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    #get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"GHG2/GHG2.csv\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checker created by Peter Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    #return words\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return word\n",
    "    #return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = lambda x : ' '.join(i for i in list(map(correct, tokens_target(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_set['Answer'] = train_set['Answer'].apply(spell_checker)\n",
    "train_set['Answer'] = train_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154115</td>\n",
       "      <td>the atmosphere because it traps in all the air...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139510</td>\n",
       "      <td>it allows sunlight to enter the earth glass al...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118519</td>\n",
       "      <td>i choose the earth because the sun reflects th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118565</td>\n",
       "      <td>it is because they acted as if they were both ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154135</td>\n",
       "      <td>it is like the geenhouse because the atmophere...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WISEID                                             Answer  KIScore\n",
       "0  154115  the atmosphere because it traps in all the air...        3\n",
       "1  139510  it allows sunlight to enter the earth glass al...        4\n",
       "2  118519  i choose the earth because the sun reflects th...        2\n",
       "3  118565  it is because they acted as if they were both ...        2\n",
       "4  154135  it is like the geenhouse because the atmophere...        2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dev_set['Answer'] = dev_set['Answer'].apply(spell_checker)\n",
    "dev_set['Answer'] = dev_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136465</td>\n",
       "      <td>the atmosphere keeps the heat inside of earth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150125</td>\n",
       "      <td>the atmosphere is like a greenhouse because th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154205</td>\n",
       "      <td>the earth takes that sunlight but sometimes al...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150173</td>\n",
       "      <td>the sunlight is all bouncing of something</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139846</td>\n",
       "      <td>heat is going inside the earth s atmosphere an...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WISEID                                             Answer  KIScore\n",
       "0  136465      the atmosphere keeps the heat inside of earth        3\n",
       "1  150125  the atmosphere is like a greenhouse because th...        2\n",
       "2  154205  the earth takes that sunlight but sometimes al...        3\n",
       "3  150173          the sunlight is all bouncing of something        3\n",
       "4  139846  heat is going inside the earth s atmosphere an...        3"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies that need to be attempted </b>\n",
    "1. Feature Selection attempted:\n",
    "    1. counts of Unigrams only \n",
    "    2. ... Unigrams and Bigrams\n",
    "    3. ... Unigrams, Bigrams, and Trigrams\n",
    "    4. ... Bigrams and Trigrams\n",
    "    5. ... 4- and 5-gram combinations\n",
    "    6. The use of TF-IDF, with IDF and without\n",
    "    7. Word tokens that included punctuation and numbers\n",
    "    8. Word tokens with letters only, filtering punctuation or splitting on punctuation\n",
    "    9. Lemmatizing using Word Net\n",
    "    10. Stemming using Snowball\n",
    "    11. With and without stopwords\n",
    "    12. With and without lowercasing\n",
    "    13. Chunking out all words that are not nouns.\n",
    "    14. Stemming user Porter and Lancaster stemmers.\n",
    "    15. Checking most common hypernyms of nouns in the review to categorise reviews better.\n",
    "    16. Using feature unions in pipelines to select specific features.\n",
    "2. Classifiers used:\n",
    "    1. Linear: Naive Bayes, Linear Regression, Stochastic Gradiant Descent\n",
    "    2. SVC and Linear SVC (One vs One, One vs Many)\n",
    "    3. K - Nearest Neighbor\n",
    "    4. MLP\n",
    "    5. Voting classifiers with hard and soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Answer\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Answer\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple Naive Bayes classifier for Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB_model(train_set, arr_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train, arr_dev = transform_dfs_to_arrays(train_set, dev_set)\n",
    "nb_model = train_NB_model(train_set, arr_train)\n",
    "nb_predictions = nb_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65535248041775462"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNearest_model(train_set, arr_train):\n",
    "    #Should add and experiement with more parameters and algorithms for nearest neighbor\n",
    "    nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54308093994778073"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_model = train_KNearest_model(train_set, arr_train)\n",
    "ne_predictions = neigh_model.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, ne_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad for a start, lets move onto Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LR_model(train_set, arr_train):\n",
    "    logreg = LogisticRegression()\n",
    "    lr_model = logreg.fit(arr_train, train_set.KIScore)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = train_LR_model(train_set, arr_train)\n",
    "lr_predictions = lr_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6866840731070496"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already nearing 80s!!!! But remember, need to measure Cohen's Kappa, not percetage correct. Also start plotting confusion matrix and extract errors once the classifiers are worked out.\n",
    "\n",
    "Lets start with the pipeline for the best features and get to feature detections using SVM. Also need to perform all the combinations mentioned before (Including preprocessing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    #this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline attempts - Best features will be decided using Grid Search. Lets just setup a baseline for now.\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#Note: add probability True to SVC classifier to be able to use predict probability function, which\n",
    "# is crucial for the the ensemble methods tried later\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\")),\n",
    "                      ('tfidf', TfidfTransformer(use_idf = True, norm='l2')),\n",
    "                      ('log', LogisticRegression(class_weight = None )),\n",
    "                      ('clf', SVC(C = 1000000.0, gamma='auto', kernel='linear', probability = True))])\n",
    "                      #('clf', LinearSVC(C=1.0, random_state=69, penalty='l2', dual=True, tol=1e-5, class_weight = None))])\n",
    "                      #('clf', OneVsOneClassifier(LinearSVC(random_state=0)))])                    \n",
    "                      #('clf', SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68407310704960833"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predictor = text_clf.fit(train_set[\"Answer\"], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = pipeline_predictor.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>130</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>51</td>\n",
       "      <td>165</td>\n",
       "      <td>127</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1    2    3   4   5  All\n",
       "True                                \n",
       "1          34    3    0   0   0   37\n",
       "2          15  130   23   2   0  170\n",
       "3           2   25   80  13   0  120\n",
       "4           0    7   20  13   5   45\n",
       "5           0    0    4   2   5   11\n",
       "All        51  165  127  30  10  383"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some ensemble classifiers (Both averaging and boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70234986945169708"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f_clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C = 100, penalty=\"l1\", dual = False))),\n",
    "  ('classification', forest_clf)\n",
    "])\n",
    "\n",
    "forest_predictor = f_clf.fit(arr_train, train_set.KIScore)\n",
    "f_predicted = forest_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, f_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51958224543080944"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "ada_predictor = ada_clf.fit(arr_train, train_set.KIScore)\n",
    "a_predicted = ada_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, a_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68407310704960833"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0).fit(arr_train, train_set.KIScore)\n",
    "grad_predictor = grad_clf.fit(arr_train, train_set.KIScore)\n",
    "grad_predicted = grad_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, grad_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And moving on to voting classifier with hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71279373368146215"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf5', clf5), ('clf8', clf8),\n",
    "                                   ('clf9', clf9)], voting='hard')\n",
    "\n",
    "eclf_predictor = eclf.fit(arr_train, train_set.KIScore)\n",
    "v_predicted = eclf_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, v_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SGD', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Voting classifier with soft voting\n",
    "\n",
    "Note: The MLP classifier improved accuracy for both hard and sofr voting\n",
    "\n",
    "But, I need to do a ton of cross validation for the correct parameters and classifiers for each question type. Not to mention, need to get the grid search working well for these things.\n",
    "\n",
    "TODO: Find optimal weights for the classifiers\n",
    "\n",
    "TODO: Need to do feature engineering to get better parameters. This isnt working too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_s = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], voting='soft')\n",
    "\n",
    "eclf_s_predictor = eclf_s.fit(arr_train, train_set.KIScore)\n",
    "s_predicted = eclf_s_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, s_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a brute force method to find the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w4', 'w5', 'w6', 'mean', 'std'))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "t0 = time()\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w4 in range(1,4):\n",
    "            for w5 in range(1,4):\n",
    "                for w6 in range(1,4):\n",
    "                        if len(set((w1,w2,w4,w5,w6))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        t0 = time()\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[w1, w2, w4, w5, w6], voting = 'soft')\n",
    "                        scores = cross_val_score(eclf, \n",
    "                                                 arr_train,\n",
    "                                                 train_set.KIScore,\n",
    "                                                 cv=3,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs= -1)\n",
    "                        \n",
    "                        print(\"done in %0.3fs\" % (time() - t0))\n",
    "                        df.loc[i] = [w1, w2, w4, w5, w6, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "                        \n",
    "#print(\"done in %0.3fs\" % (time() - t0))\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7232375979112271"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 3, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "eclf_w_predictor = eclf_w.fit(arr_train, train_set.KIScore)\n",
    "w_predicted = eclf_w_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, w_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! Not very good\n",
    "\n",
    "Lets see which categories we are getting wrong (Don't be 2!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(dev_set.KIScore, w_predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it looks like we should try and get more data. Might not be possible without mixing up student responses.\n",
    "\n",
    "To squeeze voting classifiers into the grid, I'm restricted to using only classifiers that provide a predict_pobability function. Might be worth trying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_grid = grid_search.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have exhausted all classifiers without really messing around with feature engineering or feature selection, lets add custom features to the pipeline using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Classifier for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72845953002610964"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "# Weights - 3 3 1 2 1\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),   \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7389033942558747"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Light(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('light' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'white' in [ps.stem(i) for i in text.split()])\n",
    "                and 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]    \n",
    "    \n",
    "class Keywords_Albedo(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Albedo': 'albedo' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Trap': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_light', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Light()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('key_words_albedo', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Albedo()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline2.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classifier for Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72062663185378595"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]\n",
    "                or 'sun' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Reflect(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Reflect': 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_reflect', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Reflect()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    ('feature_selection', SelectPercentile(chi2, percentile=30)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline3.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),   \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 29   8   0   0   0]\n",
      " [  1 157  12   0   0]\n",
      " [  0  34  85   1   0]\n",
      " [  0   8  33   2   2]\n",
      " [  0   1   5   2   3]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEpCAYAAAD4Vxu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHzA/Zx4IwjaLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2591419e8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(dev_set.KIScore, predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heat is going inside the earth s atmosphere and some is being trapped in\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere is like the green house window because it lets the heat in but does not let the heat out of its protective layer\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "light solar radiation does not reflect but passes through the atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the glass keeps the light from leaving the greenhouse to keep it warm and the atmosphere is full of gases that does the same prevents infrared light and heat from leaving keeping the globe warm\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the glass is like the atmosphere because while it lets the light rays pass through it also reflects some of the heat light rays back out\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "the sun came in the atmosphere and the heat from the earth is trapped in the earth s atmosphere\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere acts like the glass because it lets the sun light go in but the sunlight can t get out\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i am not really sure\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "the sun because the sun goes through the glass and shine light on the plants\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think its atmosphere because once solar radiation comes through it stays trapped and only a little gets out\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere reflects some heat and retains most heat similar to the greenhouse\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere is like the glass of the earth because it traps the heat from the sun going into the earth so the energy from the sun can t com out\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere is like a greenhouse because it traps the gases\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i chose because the atmosphere will let heat go in some it will just stay behind\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it looks the same because the sr can not escape in the greenhouse glass and the sr can not escape the atmosphere\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere lets some sun in like the glass and keeps it there because it reflects off the earth back to the atmosphere and back to earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because the\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "the atmosphere because the atmosphere in and outside of the earth protects it and allows a certain amount of sunlight and radiation caused by the sun into the earth\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it keeps the sun rays in much like the glass and allows light to come through\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "it might be\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "i chose this answer because i want to\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "the atmosphere acts like the glass of the greenhouse because it allows sun to pass through the glass atmosphere the heat that enters stays inside the atmosphere glass and will not be able to escape easily\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the sun because it goes through the glass to the plants\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "in a green house solar radiation gets sucked in then it is not allowed to leave\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i think the atmosphere because it is letting heat into to earth just like the glass is letting heat into the greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the atmosphere is like the glass of the greenhouse because the glass and the atmosphere will let some of the heat inside but will not let some of the heat leave escape\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "sdjvakk fhub\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "the atmosphere is like the glass of a green house because the heat can come in but it is hard to get out it also heats things up\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it s like a barrier for the heat to stay in\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think it s the atmosphere because the atmosphere let s heat in and prevents heat from leaving that s what happening in global warming the atmosphere is trapping to much heat and the globe is slowing warming up that s what we want our solar oven to do but not our world\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere acts like glass because it holds some of the heat inside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "well when the sun gives solar radiation out it hits the ground and then the atmosphere soaks it up keeping the heat in the atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "earth because i s the same thing as glass if a light hits earth the light will bounce just like a mirror and glass\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "i think it is the atmosphere is the glass because the glass makes all the energy s reflect the sun from the glass that is why i think that the atmosphere is the glass\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere lets sun come in and keeps the heat in\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the light goes through the atmosphere onto the earth but it does not go through back out into space and is trapping the light and heat around the globe\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "the glass is like the atmosphere sine it can contain the heat inside for the moment\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere keeps the energy inside\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere lets in the energy but when it tries to go back into space only a little bit can escape most bouncing off back to earth the atmosphere is like the greenhouse glass\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere acts as the glass of the greenhouse because they both keep the warmth in\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think that the atmosphere is like the glass in the greenhouse because the greenhouse glass absorbs the solar radiation and traps it in because the glass reflects the solar radiation just like the atmosphere the sun gives solar radiation to the earth but once it enters the atmosphere it s trapped because it reflects the solar radiation like the greenhouse glass\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "i think this because if heat could get trapped like that then it could also reflect of the plants\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere is like the glass of the greenhouse because both let radiation in and trap heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it is not allowing the heat to go out\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i chose the atmosphere because the glass and atmosphere lets hot air in and out but has the cold air stay in\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the atmosphere acts like the glass of the greenhouse because it regulates the amount of light that enters the planet the atmosphere reflects most of the sun s harmful radiation back into space while keeping the infrared radiation similar to how the glass panel does in the greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "because it does the same job as the green house gases\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "well the sun streams into the atmosphere and the it bounces of the earth then gets trapped\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere lets the heat pass through like the greenhouse glass\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the glass blocks heat from going out of the green house and the atmosphere does that to\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the sun or radiation from the sun is like entering atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the glass allows the waves to either come in or bounce back out like the atmosphere\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere reflects some sun and keeps in most just like the glass in the greenhouse\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think this because the sun shines in and stays in there\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere is like the glass in the green house because it is putting radiation in from the sun and not letting as much come out just like the glass\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "the sun keeps the greenhouse hotter because the sun has energy\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "i think that space is like the glass of the greenhouse because the layer of air covering the earth doesn t let the sun light from the sun get out of that layer of air\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it contains the warmth and rebounds the contained energy\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "i think the atmosphere is like the greenhouse glass because when the sun comes down and hits the earth the atmosphere traps the heat and stops from too much of radiation coming in\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "the green room is supposed to let the suns raise come in and keep the green room hot\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere is like the glass of the greenhouse because it lets the sunlight in but traps heat so it the inside of the greenhouse will get hotter\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "the atmosphere does not let the heat out\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think this means many things\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "the atmosphere is like the glass in picture a because it is blocking the sun waves from going any farther\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i cant explian\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "this is because the earth is helping the greenhouse heat up like the sun does\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the atmosphere allows heat into earth s atmosphere and it keeps most of the heat inside of the earth\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere acts like the glass because shows that the radiation can t escape from the atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere is the glass because it is the layer which has light coming in but also has gases that earth or the plants release trapped in\n",
      "Graded as: 2\n",
      "Actual grade is 5\n",
      "\n",
      "some sun rays are getting in the atmosphere and some are not\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere is like the glass in the green house because when the sun rays travel through it they become heat when they rise up into the air they can t escape because the atmosphere is holding them back\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "just like the glass the atmosphere lets the light in but doesn t let it out\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "the glass of the greenhouse is like the atmosphere of earth because it shows the energy from the sun trying to escape earths atmosphere on the greenhouse picture it shows the energy from the sun transferring to the greenhouse and the energy trying to get out and escape but the solar planet isn t letting it\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the sun is the main energy of a sun house\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the atmosphere is where the heat source stays and bounces back\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere is like a glass wall it secures the earth making sure only light comes in but its very fragile\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the heat can t escape through the atmosphere\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the atmosphere is like the glass of the greenhouse because once the sun goes into the atmosphere the heat is trapped inside\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it is because the atmosphere show how the heat goes in and doe not get out\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i think the atmosphere because the sun s energy reflects off the atmosphere into the earth s core\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the heat is passing through the glass like the heat passing through the atmosphere\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because their both green so they are like sister and brothers\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "it s the closes thing to the radiation point\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the atmosphere is like the glass of a greenhouse because the heat can come inside it but it can not get out it just bounces off the atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "because the atmosphere serves as a window for earth it is the same thing but bigger\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think the atmosphere is the glass because the heat rays are going through it and hitting the earth plant\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere is like the glass because it absorbs uv light and it hovers over and all around earth just like the glass\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "its like the glass because heat from sun is getting absorbed\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "just like the atmosphere the glass traps in the solar radiation that comes from the sun which makes the room hotter\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere keeps heat in just like the glass keeps heat in\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the earth absorbs the sun witch is like how the green houses glass keeps it in\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think it is the suns reflection and the greenhouse keeps the sun in and creates evaporation\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it keeps the solar radiation in the earth like glass\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the heat can go into he atmosphere but can t leave that is the same with the green house it can go through the glass but can t leave\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere layer above the earth is clear like glass and the sun s light rays are able to penetrate through it because of its transparency\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "this is because the atmosphere acts as a mirror because it has molecules that trap sunlight in\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i chose this awnser because in the picture of the earth it has the light bouncing off the glass acting as earth\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the glass is the atmosphere because it allows sunlight to travel in but also trapping it inside the earth greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "because in the picture i see that the earth is trapping the heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the atmosphere is acting like the glass because it is conducting the sunlight and preventing it from leaving\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the glass of the greenhouse and the atmosphere both let controlled amounts of heat through and after it makes it s way through the atmosphere or glass it cannot come back out again\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "in picture a it shows that the sun rays go through the glass in picture b it shows the sun rays go through the atmosphere which then hits the earth\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere is like a bubble surrounding earth and acts like the glass of a greenhouse it acts to contain the heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the sun in both pictures are both yellow and the sun goes in to the green house like the sun goes on to the earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the sun is going through the windows and the sun is going to the plants transfering energy to the plants\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i chose atmosphere because it is keeping it in not he earth because it is on the inside like the plants so the atmosphere is more like the glass for that reason the sun cannot be it because it is not around the earth and space does not have a type of gravity on earth that can keep the radiation inside the atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[2]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[1]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[2])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prediction probabilities to see confidence level of predictions and defer to manual grader if needed. Work with the prediction probability feature of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56281803731637847"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "               ('eclf2', pipeline2),\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'eclf2__transformer_weights_body_stats': (0.5, 0.75, 1.0),\n",
    "    'eclf2__transformer_key_words_dark': (0.5, 0.75, 1.0)\n",
    "}\n",
    "\n",
    "#parameters = {}\n",
    "\n",
    "grid_search = GridSearchCV( pipe3, parameters, n_jobs=-1, verbose=1, cv = 3)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe3.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_set[\"Answer\"], train_set.KIScore)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
