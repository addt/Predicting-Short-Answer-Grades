{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the pre and post test data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the '|' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, encoding = 'mbcs')\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['KIScore'] = df['KIScore'].astype(int)\n",
    "    df['Answer'] = df['Answer'].astype(str)\n",
    "    # Filters if needed later on\n",
    "    #filtered_data = df[\"Answer\"].notnull()\n",
    "    #filtered_data = df[df[\"KIScore\"] != 1 & df['Answer'].notnull() & df[\"KIScore\"].notnull()]\n",
    "    #df_narrative = df[filtered_data]\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda - Steve \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    #print the report on category breakdown, might need these counts later\n",
    "    #print(\"Creating training data... category breakdown:\")\n",
    "    #sorted_product_counts = df_narrative.Category.value_counts(ascending=True)\n",
    "    #print(sorted_product_counts)\n",
    "    #sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Categories\");\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into training and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set - Currently only genereates dev and test data\n",
    "#Modify the function later to keep some data as test data as well\n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    #randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['WISEID', 'Answer', 'KIScore']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    #separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    #get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"GHG2/GHG2.csv\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checker created by Peter Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    return words\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return word\n",
    "    #return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = lambda x : ' '.join(i for i in list(map(correct, tokens_target(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_set['Answer'] = train_set['Answer'].apply(spell_checker)\n",
    "train_set['Answer'] = train_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118579</td>\n",
       "      <td>atmosphere is glass letting gas</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139509</td>\n",
       "      <td>heat is trapped atmosphere greenhouse gases re...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139481</td>\n",
       "      <td>glass greenhouse atmosphere is letting heat en...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139840</td>\n",
       "      <td>atmospheres prohibits sun release heat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139834</td>\n",
       "      <td>acts insulator keeps heat earth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WISEID                                             Answer  KIScore\n",
       "0  118579                    atmosphere is glass letting gas        2\n",
       "1  139509  heat is trapped atmosphere greenhouse gases re...        4\n",
       "2  139481  glass greenhouse atmosphere is letting heat en...        4\n",
       "3  139840             atmospheres prohibits sun release heat        2\n",
       "4  139834                    acts insulator keeps heat earth        3"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dev_set['Answer'] = dev_set['Answer'].apply(spell_checker)\n",
    "dev_set['Answer'] = dev_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154123.0</td>\n",
       "      <td>because it traps the heat inside so it wont es...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154314.0</td>\n",
       "      <td>The atmosphere traps heat from the sun and car...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139856.0</td>\n",
       "      <td>The glass reflects and/or absorbs heat through...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154148.0</td>\n",
       "      <td>Because the atmosphere takes some radiation an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139537.0</td>\n",
       "      <td>Atmosphere has greenhouse gases which reflects...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  154123.0  because it traps the heat inside so it wont es...        3\n",
       "1  154314.0  The atmosphere traps heat from the sun and car...        3\n",
       "2  139856.0  The glass reflects and/or absorbs heat through...        4\n",
       "3  154148.0  Because the atmosphere takes some radiation an...        4\n",
       "4  139537.0  Atmosphere has greenhouse gases which reflects...        4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies that need to be attempted </b>\n",
    "1. Feature Selection attempted:\n",
    "    1. counts of Unigrams only \n",
    "    2. ... Unigrams and Bigrams\n",
    "    3. ... Unigrams, Bigrams, and Trigrams\n",
    "    4. ... Bigrams and Trigrams\n",
    "    5. ... 4- and 5-gram combinations\n",
    "    6. The use of TF-IDF, with IDF and without\n",
    "    7. Word tokens that included punctuation and numbers\n",
    "    8. Word tokens with letters only, filtering punctuation or splitting on punctuation\n",
    "    9. Lemmatizing using Word Net\n",
    "    10. Stemming using Snowball\n",
    "    11. With and without stopwords\n",
    "    12. With and without lowercasing\n",
    "    13. Chunking out all words that are not nouns.\n",
    "    14. Stemming user Porter and Lancaster stemmers.\n",
    "    15. Checking most common hypernyms of nouns in the review to categorise reviews better.\n",
    "    16. Using feature unions in pipelines to select specific features.\n",
    "2. Classifiers used:\n",
    "    1. Linear: Naive Bayes, Linear Regression, Stochastic Gradiant Descent\n",
    "    2. SVC and Linear SVC (One vs One, One vs Many)\n",
    "    3. K - Nearest Neighbor\n",
    "    4. MLP\n",
    "    5. Voting classifiers with hard and soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Answer\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Answer\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple Naive Bayes classifier for Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB_model(train_set, arr_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train, arr_dev = transform_dfs_to_arrays(train_set, dev_set)\n",
    "nb_model = train_NB_model(train_set, arr_train)\n",
    "nb_predictions = nb_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66057441253263705"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNearest_model(train_set, arr_train):\n",
    "    #Should add and experiement with more parameters and algorithms for nearest neighbor\n",
    "    nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53524804177545693"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_model = train_KNearest_model(train_set, arr_train)\n",
    "ne_predictions = neigh_model.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, ne_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad for a start, lets move onto Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LR_model(train_set, arr_train):\n",
    "    logreg = LogisticRegression()\n",
    "    lr_model = logreg.fit(arr_train, train_set.KIScore)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = train_LR_model(train_set, arr_train)\n",
    "lr_predictions = lr_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70234986945169708"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already nearing 80s!!!! But remember, need to measure Cohen's Kappa, not percetage correct. Also start plotting confusion matrix and extract errors once the classifiers are worked out.\n",
    "\n",
    "Lets start with the pipeline for the best features and get to feature detections using SVM. Also need to perform all the combinations mentioned before (Including preprocessing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    #this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline attempts - Best features will be decided using Grid Search. Lets just setup a baseline for now.\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#Note: add probability True to SVC classifier to be able to use predict probability function, which\n",
    "# is crucial for the the ensemble methods tried later\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\")),\n",
    "                      ('tfidf', TfidfTransformer(use_idf = True, norm='l2')),\n",
    "                      ('log', LogisticRegression(class_weight = None )),\n",
    "                      ('clf', SVC(C = 1000000.0, gamma='auto', kernel='linear', probability = True))])\n",
    "                      #('clf', LinearSVC(C=1.0, random_state=69, penalty='l2', dual=True, tol=1e-5, class_weight = None))])\n",
    "                      #('clf', OneVsOneClassifier(LinearSVC(random_state=0)))])                    \n",
    "                      #('clf', SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66057441253263705"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predictor = text_clf.fit(train_set[\"Answer\"], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = pipeline_predictor.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>48</td>\n",
       "      <td>174</td>\n",
       "      <td>110</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1    2    3   4   5  All\n",
       "True                                \n",
       "1          31    2    0   0   0   33\n",
       "2          16  132   24   4   0  176\n",
       "3           0   30   65  11   2  108\n",
       "4           1    9   16  20   3   49\n",
       "5           0    1    5   6   5   17\n",
       "All        48  174  110  41  10  383"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some ensemble classifiers (Both averaging and boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6866840731070496"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f_clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C = 100, penalty=\"l1\", dual = False))),\n",
    "  ('classification', forest_clf)\n",
    "])\n",
    "\n",
    "forest_predictor = f_clf.fit(arr_train, train_set.KIScore)\n",
    "f_predicted = forest_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, f_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54046997389033946"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "ada_predictor = ada_clf.fit(arr_train, train_set.KIScore)\n",
    "a_predicted = ada_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, a_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69190600522193213"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0).fit(arr_train, train_set.KIScore)\n",
    "grad_predictor = grad_clf.fit(arr_train, train_set.KIScore)\n",
    "grad_predicted = grad_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, grad_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And moving on to voting classifier with hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71279373368146215"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf5', clf5), ('clf8', clf8),\n",
    "                                   ('clf9', clf9)], voting='hard')\n",
    "\n",
    "eclf_predictor = eclf.fit(arr_train, train_set.KIScore)\n",
    "v_predicted = eclf_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, v_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SGD', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Voting classifier with soft voting\n",
    "\n",
    "Note: The MLP classifier improved accuracy for both hard and sofr voting\n",
    "\n",
    "But, I need to do a ton of cross validation for the correct parameters and classifiers for each question type. Not to mention, need to get the grid search working well for these things.\n",
    "\n",
    "TODO: Find optimal weights for the classifiers\n",
    "\n",
    "TODO: Need to do feature engineering to get better parameters. This isnt working too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_s = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], voting='soft')\n",
    "\n",
    "eclf_s_predictor = eclf_s.fit(arr_train, train_set.KIScore)\n",
    "s_predicted = eclf_s_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, s_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a brute force method to find the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w4', 'w5', 'w6', 'mean', 'std'))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "t0 = time()\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w4 in range(1,4):\n",
    "            for w5 in range(1,4):\n",
    "                for w6 in range(1,4):\n",
    "                        if len(set((w1,w2,w4,w5,w6))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        t0 = time()\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[w1, w2, w4, w5, w6], voting = 'soft')\n",
    "                        scores = cross_val_score(eclf, \n",
    "                                                 arr_train,\n",
    "                                                 train_set.KIScore,\n",
    "                                                 cv=3,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs= -1)\n",
    "                        \n",
    "                        print(\"done in %0.3fs\" % (time() - t0))\n",
    "                        df.loc[i] = [w1, w2, w4, w5, w6, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "                        \n",
    "#print(\"done in %0.3fs\" % (time() - t0))\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7389033942558747"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 3, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "eclf_w_predictor = eclf_w.fit(arr_train, train_set.KIScore)\n",
    "w_predicted = eclf_w_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, w_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! Not very good\n",
    "\n",
    "Lets see which categories we are getting wrong (Don't be 2!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(dev_set.KIScore, w_predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it looks like we should try and get more data. Might not be possible without mixing up student responses.\n",
    "\n",
    "To squeeze voting classifiers into the grid, I'm restricted to using only classifiers that provide a predict_pobability function. Might be worth trying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_grid = grid_search.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have exhausted all classifiers without really messing around with feature engineering or feature selection, lets add custom features to the pipeline using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Classifier for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71279373368146215"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "# Weights - 3 3 1 2 1\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),   \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71279373368146215"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Light(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('light' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'white' in [ps.stem(i) for i in text.split()])\n",
    "                and 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]    \n",
    "    \n",
    "class Keywords_Albedo(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Albedo': 'albedo' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Trap': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_light', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Light()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('key_words_albedo', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Albedo()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline2.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classifier for Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70496083550913835"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]\n",
    "                or 'sun' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Reflect(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Reflect': 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_reflect', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Reflect()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    ('feature_selection', SelectPercentile(chi2, percentile=30)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline3.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 26   7   0   0   0]\n",
      " [  0 164  11   0   1]\n",
      " [  0  36  72   0   0]\n",
      " [  0  11  30   6   2]\n",
      " [  0   0  12   4   1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEpCAYAAAD4Vxu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOedC8oTpnHMh/T8Fv8NcEfYOjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25913a219e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(dev_set.KIScore, predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The glass reflects and/or absorbs heat through radiation. We know this because the light arrows are shining through the glass just like the earth's atmosphere.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Because the atmosphere takes some radiation and kicks the others out.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "This is because if you have sand touch something hot like the sun it will make glass.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "The glass at the greenhouse gasses keeps the hot air in along with it allowing it to get into the greenhouse.  This is the roll the the atmosphere plays for the Earth.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere that surrounds the Earth and keeps the hot air inside is like the glass of the greenhouse that keeps the hot air inside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "11 is the new F\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "The atmosphere traps the energy.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere because the atmosphere in and outside of the Earth protects it and allows a certain amount of sunlight and radiation caused by the sun into the Earth.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I think that the glass is most like the atmosphere because the heat goes through the glass and then is tapped in whle on arth the heat hits the land then rises into the air\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "They both let radiation in and once the radiation transforms into heat, it traps the heat in\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "The atmosphere is like the glass of the greenhouse because the atmosphere keeps the warm air enclosed so it doesn't escape. \n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      " in a green house solar radiation  gets sucked in then it is not allowed to leave.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Because You see a heat wave in the picture and it uses light\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because the atmosphere reflects heat all around earth and provides plants energy and animals and people heat.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Carbon and other greenhouse gases in the atmosphere act like Plexiglas letting light in and some light out.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere holds all the heat in in so it does not bounce off .\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "In the green house there is glass that keeps the heat trapped in, and in the Picture \"B\" the atmosphere seems to be doing the same thing. \n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "porquecalienta\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "The atmosphere allows sun light to pass through and heating the earth. The atmosphere can also prevent heat energy from escaping into space.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "I think so because once heat comes in it can't get out.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The sun is most like the glass of the greenhouse, because both the glass and the atmosphere allow the heat to pass through, but trap the heat, because the glass traps heat to give sunlight to the plants, and the atmosphere traps heat to give warmth and sunlight to the Earth.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "Like the greenhouse panels, the atmosphere prevents the heat energy from escaping and keeps it trapped within the earth.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "It allows sunlight to enter the Earth. Glass allows sunlight to enter the greenhouse.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I think that the atmosphere is like the glass of the greenhouse because you can see from the pictures that the sun radiation is coming in and out through out the glass at the top of the green house.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "really don't know it was a guess\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "The atmosphere is like the glass of the greenhouse because it keep the heat energy from escaping same as the glass in the greenhouse.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere acts like the glass, because tthe sunrays go throught the glass, as it does throught the atosphere\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The glass lets the light in, but does not let the heat escape. That is like what the atmosphere does.\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "because they both absorb the heat and let it in.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Both the atmosphere and glass allows the light/heat to come in but also reflects back some of the light/heat.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "Earth because it keeps the heat like the greenhouse.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The energy is trapped in the atmosphere because the energy is right around the atmosphere.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Light/Solar radiation does not reflect but passes through the atmosphere.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "On picture (a) it shows you how the glas keeps the heat in well it works the same way with the earth.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The picture show how the atmosphere traps the sun like a greenhouse does\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the sun is like the grass of the green house because the light is going into the green house and is trapped and that's whats happening also to the atmosphere. \n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere will allow the sun's radiation pass through the atmosphere but will not allow heat leave the Earth.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "Earth is the planet, which is the one who bounces the heat. The sun is 93 million miles away from Earth, and is the one radiating heat. Space is the place where the heat travels through.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Just how the sun passes through the glass panels , the sun's waves passes through the atmosphere.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere is like a glass wall it secures the earth making sure only light comes in but its very fragile.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I chose atmosphere because when the light passes through the glass, it can't really escape, so when the arrows touch the thermosphere, it bounces back towards the Earth again.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "It is he atmosphere because the Atmosphere collects heat and traps it inside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "It keeps the sun rays in, much like the glass, and allows light to come through.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "The atmosphere is like the glass in the green house because when the sun rays travel through it, they become heat. When they rise up into the air, they can't escape because the atmosphere is holding them back.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "The atmosphere acts as the greenhouse where light passes through it and heats up the earth.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "It is like the atmosphere because it releases and let then sun rays enter like the atmosphere. It is the thing that keeps the inside warm and captures the heat in the inside. It has the same process.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere is like the glass of the greenhouse because it accepts light but does not let the heat go out. This is due to the gases in the atmosphere.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "Atmosphere determines how much heat should be inside the Earth, it keeps tempatures ideal for life on Earth and lets heat leave Earth too, but it could only be little heat.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The atmosphere stops heat from escaping and lets the solar radiation come in.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "The sun passes through the atmosphere easily but traps it inside;greenhouse effect thus warming up our planet and acting like the glass of the greenhouse.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "i cant explian\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "The atmosphere refelcts the ligth back into space, but stores a little part of the radaion\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because the heat gets in but can't get out\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere keeps heat in just like the glass keeps heat in.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Because the atmosphere contains molecules and greenhouses gases that either absorb or reflect the heat back.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Earths atmosphere holds in the heat, just like the glass of a greenhouse.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere allows solar radiation to enter and prevents infrared radiation to exit. The glass of our solar oven behaves the same way.\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere absorbs the heat and reflects it. The absorbed heat warms Earth up.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because sun can go through it but not come out.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "It looks the same because the SR can not escape in the greenhouse glass and the SR can not escape the atmosphere.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "We can see that in the atmosphere, it is where the heat is bouncing back towards the earth, just like the other diagram where the glass if forcing the heat to bounce back to the plant.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "I think that the Sun is like the greenhouse because the heat of the sun goes right in the glass.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "just a guess i don't know\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "i think the atmosphere because the is make the sun not escape the earth.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "So the glass of the greenhouse keeps the heat and solar radiation in, but so does the atmoshpere.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere is like the glass of a greenhouse because it's like it bouncing back inside of the atmosphere like the heat does in a greenhouse.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Picture B is like the green house because it is similar to the suns raise bouncing off the earth.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Like the glass, the atmosphere traps the infrared radiation inside the atmosphere, warming the Earth.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "The atmosphere is light the glass of the greenhouse because once sunlight travels through it, it traps most of the heat in, keeping Earth warm.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "Atmosphere contains air and keeps in, being warmed by sun\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "When the solar radiation passes through the glass of the greenhouse, it becomes infrared radiation.It is very similar to what the atmosphere might do.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere is like the glass because it does the same thing as the glass does it blocks out harmful rays of sun but letting some of it in to heat up the earth.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "it the sun because in the green house the sun put the heat inside of it\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The Atmosphere is like a big blanket around the Earth and when the sun's rays come in they can't get out because of this big giant blanket in the way (the atmosphere) and the glass panels in the green house are like the the big giant blanket around the green house that let's the sun's rays out and not back in just like the Atmosphere does.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "I chose this answer because I want to.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "the atmosphere has gases that bounce back the rays and the ray go through and might not go out.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "The glass of the greenhouse traps the necessary amount of heat that will fit the amount that the plants need, and reflects the amount that the plants do not need. Like the glass, the atmosphere allows a certain amount of sun rays to penetrate the surface to warm up the surface and the ground of the Earth, but the left over amount that is not needed is reflected to space.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere keeps hot air from coming out of earth.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The sun is giving energy to the plants and is getting trapped in the greenhouse.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The earth is absorbing some of the heat/sunlight but not all like the greenhouse.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere serves to trap heat energy on the earth just like the glass of the greenhouse.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the atmosphere let the light in just like the glass and also allowed some light to exit and kept some light just like the glass\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The glass and atmosphere both traps in heat energy.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I picked atmosphere because just like the greenhouse glass, the heat goes through the atmosphere.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Some of the light from the sun is passing into the greenhouse through the sun, and some is getting reflected. The same thing is happening with the sunlight passing through the atmosphere and into the earth.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The Sun because, it shows how heat comes through the atmosphere ad the glass, but bounce off it and never gets out.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The heat from the sun enters the atmosphere and the atmosphere prevents the heat from escaping.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The atmosphere is the outer ring of the earth. So in the picture showed the light bouncing off a  ring outside of the earth so I took a guess that it is the atmosphere.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The glass acts like the atmosphere because heat come through it but cannot escape.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "I said space is the answer because the space is reflecting the energy source to go back to the earth and heat it up.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "In the picture the Earth is reflecting the heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere on earth is what allows the rays to enter, and what prevents rays from leaving. It is like a filter, similar to the glass which traps in heat but does not allow it to escape so that plants could grow. \n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "i chose the atmosphere because it is like glass keeping the heats in.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because the heat gets trapped in and can't get out.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Just like the glass in the picture the atmosphere keeps the radiation in.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere makes it so the heat cannot escape and it makes it so it deflects the heat so it 'bounces' back to the Earth.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere lets air come in but it doesn't let it come out\n",
      "Graded as: 5\n",
      "Actual grade is 2\n",
      "\n",
      "The  warm air is traped in by the atmosphere surounding the earth.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere is like a roof around the Earth so the heat wont go out.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere reflects some sun and keeps in most, just like the glass in the greenhouse\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The atmosphere bounces the sunrays back to Earth, like the glass bring the sunrays back to the plants.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the sun is reflecting the sun beam of light will go throw the glass and the plants will grow in side and not in side\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere is like the glass in the green house because it is putting radiation in from the sun and not letting as much come out just like the glass.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "this allows light to come into earth that will later be transfered into light\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The atmosphere is like glass of a greenhouse because it lets radiation travel in, but then traps it in.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "The glass of the greenhouse is like the atmosphere of earth because it shows the energy from the sun trying to escape earths atmosphere. On the greenhouse picture it shows the energy from the sun transferring to the greenhouse and the energy trying to get out and escape but the solar planet isn't letting it.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere works like the window because the energy can get through it and heat up the inside and it also reflects off the surface.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The sun shoots sun rays and makes the greenhouse hot because of sun rays but the heat cant get up.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere doesn't let oxygen escape.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The atmosphere acts as the glass in the greenhouse, it allows heat to come in and lets less out.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "I think it is the atmosphere because it is above earth and because I heard that one of the atmosphere's layers is really hot because it traps the energy.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The sun is going through the glass and getting into the warm greenhouse to give the plants light.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The atmosphere is like the glass of the greenhouse because, the glass and the atmosphere will let some of the heat inside, but will not let some of the heat, leave/escape.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "It shows the sun giving and getting energy from the green\n",
      " house\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[2]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[1]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[2])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prediction probabilities to see confidence level of predictions and defer to manual grader if needed. Work with the prediction probability feature of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53255179058936886"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "               ('eclf2', pipeline2),\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'eclf2__transformer_weights_body_stats': (0.5, 0.75, 1.0),\n",
    "    'eclf2__transformer_key_words_dark': (0.5, 0.75, 1.0)\n",
    "}\n",
    "\n",
    "#parameters = {}\n",
    "\n",
    "grid_search = GridSearchCV( pipe3, parameters, n_jobs=-1, verbose=1, cv = 3)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe3.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_set[\"Answer\"], train_set.KIScore)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
