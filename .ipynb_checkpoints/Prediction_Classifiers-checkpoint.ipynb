{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the pre and post test data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the '|' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, encoding = 'mbcs')\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['KIScore'] = df['KIScore'].astype(int)\n",
    "    df['Answer'] = df['Answer'].astype(str)\n",
    "    # Filters if needed later on\n",
    "    #filtered_data = df[\"Answer\"].notnull()\n",
    "    #filtered_data = df[df[\"KIScore\"] != 1 & df['Answer'].notnull() & df[\"KIScore\"].notnull()]\n",
    "    #df_narrative = df[filtered_data]\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda - Steve \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    #print the report on category breakdown, might need these counts later\n",
    "    #print(\"Creating training data... category breakdown:\")\n",
    "    #sorted_product_counts = df_narrative.Category.value_counts(ascending=True)\n",
    "    #print(sorted_product_counts)\n",
    "    #sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Categories\");\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into training and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set - Currently only genereates dev and test data\n",
    "#Modify the function later to keep some data as test data as well\n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    #randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['WISEID', 'Answer', 'KIScore']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    #separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    #get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"Tomato/Tomato.csv\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checker created by Peter Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    return words\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return word\n",
    "    #return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = lambda x : ' '.join(i for i in list(map(correct, tokens_target(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_set['Answer'] = train_set['Answer'].apply(spell_checker)\n",
    "train_set['Answer'] = train_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154124.0</td>\n",
       "      <td>when building i will need to think abut what m...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154131.0</td>\n",
       "      <td>what i would do is get solar panels and put th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149956.0</td>\n",
       "      <td>to make it hotter you will have to capture the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139802.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154149.0</td>\n",
       "      <td>you will need a giant area for the tomatoes in...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  154124.0  when building i will need to think abut what m...        4\n",
       "1  154131.0  what i would do is get solar panels and put th...        2\n",
       "2  149956.0  to make it hotter you will have to capture the...        4\n",
       "3  139802.0                                                nan        1\n",
       "4  154149.0  you will need a giant area for the tomatoes in...        3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dev_set['Answer'] = dev_set['Answer'].apply(spell_checker)\n",
    "dev_set['Answer'] = dev_set['Answer'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150060.0</td>\n",
       "      <td>you should put it in a spot with sun this will...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154136.0</td>\n",
       "      <td>you will need a heater to make it hot in side ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136460.0</td>\n",
       "      <td>well tomatoes can t have it to cold or to hot ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139871.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150124.0</td>\n",
       "      <td>if you put the tomatoes in a greenhouse then i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  150060.0  you should put it in a spot with sun this will...        3\n",
       "1  154136.0  you will need a heater to make it hot in side ...        2\n",
       "2  136460.0  well tomatoes can t have it to cold or to hot ...        3\n",
       "3  139871.0                                                nan        1\n",
       "4  150124.0  if you put the tomatoes in a greenhouse then i...        3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies that need to be attempted </b>\n",
    "1. Feature Selection attempted:\n",
    "    1. counts of Unigrams only \n",
    "    2. ... Unigrams and Bigrams\n",
    "    3. ... Unigrams, Bigrams, and Trigrams\n",
    "    4. ... Bigrams and Trigrams\n",
    "    5. ... 4- and 5-gram combinations\n",
    "    6. The use of TF-IDF, with IDF and without\n",
    "    7. Word tokens that included punctuation and numbers\n",
    "    8. Word tokens with letters only, filtering punctuation or splitting on punctuation\n",
    "    9. Lemmatizing using Word Net\n",
    "    10. Stemming using Snowball\n",
    "    11. With and without stopwords\n",
    "    12. With and without lowercasing\n",
    "    13. Chunking out all words that are not nouns.\n",
    "    14. Stemming user Porter and Lancaster stemmers.\n",
    "    15. Checking most common hypernyms of nouns in the review to categorise reviews better.\n",
    "    16. Using feature unions in pipelines to select specific features.\n",
    "2. Classifiers used:\n",
    "    1. Linear: Naive Bayes, Linear Regression, Stochastic Gradiant Descent\n",
    "    2. SVC and Linear SVC (One vs One, One vs Many)\n",
    "    3. K - Nearest Neighbor\n",
    "    4. MLP\n",
    "    5. Voting classifiers with hard and soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Answer\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Answer\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple Naive Bayes classifier for Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB_model(train_set, arr_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train, arr_dev = transform_dfs_to_arrays(train_set, dev_set)\n",
    "nb_model = train_NB_model(train_set, arr_train)\n",
    "nb_predictions = nb_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59385665529010234"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNearest_model(train_set, arr_train):\n",
    "    #Should add and experiement with more parameters and algorithms for nearest neighbor\n",
    "    nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52586206896551724"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_model = train_KNearest_model(train_set, arr_train)\n",
    "ne_predictions = neigh_model.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, ne_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad for a start, lets move onto Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LR_model(train_set, arr_train):\n",
    "    logreg = LogisticRegression()\n",
    "    lr_model = logreg.fit(arr_train, train_set.KIScore)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = train_LR_model(train_set, arr_train)\n",
    "lr_predictions = lr_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66666666666666663"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already nearing 80s!!!! But remember, need to measure Cohen's Kappa, not percetage correct. Also start plotting confusion matrix and extract errors once the classifiers are worked out.\n",
    "\n",
    "Lets start with the pipeline for the best features and get to feature detections using SVM. Also need to perform all the combinations mentioned before (Including preprocessing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    #this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline attempts - Best features will be decided using Grid Search. Lets just setup a baseline for now.\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#Note: add probability True to SVC classifier to be able to use predict probability function, which\n",
    "# is crucial for the the ensemble methods tried later\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\")),\n",
    "                      ('tfidf', TfidfTransformer(use_idf = True, norm='l2')),\n",
    "                      ('log', LogisticRegression(class_weight = None )),\n",
    "                      ('clf', SVC(C = 1000000.0, gamma='auto', kernel='linear', probability = True))])\n",
    "                      #('clf', LinearSVC(C=1.0, random_state=69, penalty='l2', dual=True, tol=1e-5, class_weight = None))])\n",
    "                      #('clf', OneVsOneClassifier(LinearSVC(random_state=0)))])                    \n",
    "                      #('clf', SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.56313993174061439"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predictor = text_clf.fit(train_set[\"Answer\"], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = pipeline_predictor.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>72</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>33</td>\n",
       "      <td>90</td>\n",
       "      <td>117</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1   2    3   4  5  All\n",
       "True                              \n",
       "1          18   3    1   0  0   22\n",
       "2          11  54   29   3  0   97\n",
       "3           3  31   72  20  2  128\n",
       "4           1   2   15  21  2   41\n",
       "5           0   0    0   5  0    5\n",
       "All        33  90  117  49  4  293"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some ensemble classifiers (Both averaging and boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66552901023890787"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f_clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C = 100, penalty=\"l1\", dual = False))),\n",
    "  ('classification', forest_clf)\n",
    "])\n",
    "\n",
    "forest_predictor = f_clf.fit(arr_train, train_set.KIScore)\n",
    "f_predicted = forest_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, f_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4885057471264368"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "ada_predictor = ada_clf.fit(arr_train, train_set.KIScore)\n",
    "a_predicted = ada_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, a_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65804597701149425"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0).fit(arr_train, train_set.KIScore)\n",
    "grad_predictor = grad_clf.fit(arr_train, train_set.KIScore)\n",
    "grad_predicted = grad_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, grad_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And moving on to voting classifier with hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64846416382252559"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf5', clf5), ('clf8', clf8),\n",
    "                                   ('clf9', clf9)], voting='hard')\n",
    "\n",
    "eclf_predictor = eclf.fit(arr_train, train_set.KIScore)\n",
    "v_predicted = eclf_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, v_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SGD', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Voting classifier with soft voting\n",
    "\n",
    "Note: The MLP classifier improved accuracy for both hard and sofr voting\n",
    "\n",
    "But, I need to do a ton of cross validation for the correct parameters and classifiers for each question type. Not to mention, need to get the grid search working well for these things.\n",
    "\n",
    "TODO: Find optimal weights for the classifiers\n",
    "\n",
    "TODO: Need to do feature engineering to get better parameters. This isnt working too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_s = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], voting='soft')\n",
    "\n",
    "eclf_s_predictor = eclf_s.fit(arr_train, train_set.KIScore)\n",
    "s_predicted = eclf_s_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, s_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a brute force method to find the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w4', 'w5', 'w6', 'mean', 'std'))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "t0 = time()\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w4 in range(1,4):\n",
    "            for w5 in range(1,4):\n",
    "                for w6 in range(1,4):\n",
    "                        if len(set((w1,w2,w4,w5,w6))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        t0 = time()\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[w1, w2, w4, w5, w6], voting = 'soft')\n",
    "                        scores = cross_val_score(eclf, \n",
    "                                                 arr_train,\n",
    "                                                 train_set.KIScore,\n",
    "                                                 cv=3,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs= -1)\n",
    "                        \n",
    "                        print(\"done in %0.3fs\" % (time() - t0))\n",
    "                        df.loc[i] = [w1, w2, w4, w5, w6, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "                        \n",
    "#print(\"done in %0.3fs\" % (time() - t0))\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6450511945392492"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 0)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 5, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "eclf_w_predictor = eclf_w.fit(arr_train, train_set.KIScore)\n",
    "w_predicted = eclf_w_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, w_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! Not very good\n",
    "\n",
    "Lets see which categories we are getting wrong (Don't be 2!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(dev_set.KIScore, w_predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it looks like we should try and get more data. Might not be possible without mixing up student responses.\n",
    "\n",
    "To squeeze voting classifiers into the grid, I'm restricted to using only classifiers that provide a predict_pobability function. Might be worth trying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_grid = grid_search.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have exhausted all classifiers without really messing around with feature engineering or feature selection, lets add custom features to the pipeline using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Classifier for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72845953002610964"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "# Weights - 3 3 1 2 1\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),   \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7389033942558747"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Light(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('light' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'white' in [ps.stem(i) for i in text.split()])\n",
    "                and 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]    \n",
    "    \n",
    "class Keywords_Albedo(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Albedo': 'albedo' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Trap': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_light', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Light()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('key_words_albedo', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Albedo()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline2.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classifier for Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55235602094240843"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]\n",
    "                or 'sun' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Reflect(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Reflect': 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_reflect', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Reflect()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    ('feature_selection', SelectPercentile(chi2, percentile=30)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline3.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76501305483028725"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Word_All(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'All': 'all' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'both' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_all', Pipeline([ # Give low weight\n",
    "                    ('All', Word_All()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),\n",
    "    ('feature_selection', SelectPercentile(chi2, percentile=30)),\n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Question 5 Classifier\n",
    "\n",
    "Simple hard or soft voting works better than with feature selections/engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57471264367816088"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),\n",
    "    #('feature_selection', SelectPercentile(chi2, percentile=80)),\n",
    "    #('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 27   3   1   0   0]\n",
      " [  1  47  70   0   0]\n",
      " [  0  32 114   6   1]\n",
      " [  0   3  24  11   0]\n",
      " [  0   0   5   2   1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEpCAYAAAD4Vxu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18dd0c617f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(dev_set.KIScore, predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probab = p_predictor.predict_proba(dev_set['Answer'].values)\n",
    "count = 0\n",
    "conf = pd.DataFrame()\n",
    "uncern = pd.DataFrame()\n",
    "for i in range(0, len(probab)):\n",
    "    filt = list(filter(lambda x: x > 0.6, probab[i]))\n",
    "    if len(filt) > 0:\n",
    "        df1 = pd.DataFrame([list(dev_set.ix[i])], columns = ['WISEID', 'Answer', 'KIScore'])\n",
    "        #print(df1)\n",
    "        conf = conf.append(df1)\n",
    "    else:\n",
    "        df1 = pd.DataFrame([list(dev_set.ix[i])], columns = ['WISEID', 'Answer', 'KIScore'])\n",
    "        uncern = uncern.append(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71764705882352942"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ch = p_predictor.predict(conf['Answer'].values)\n",
    "accuracy_score(conf.KIScore, predicted_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59402985074626868"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(conf.KIScore, predicted_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[25  3  0  0  0]\n",
      " [ 0 12 16  0  0]\n",
      " [ 0  0 24  0  0]\n",
      " [ 0  0  4  0  0]\n",
      " [ 0  0  1  0  0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEpCAYAAADFxXrQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18db2d24438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(conf.KIScore, predicted_ch)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atmosphere holds heat greenhouse glass\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "is glass reflects lets glass doses\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere locks radiation sun other words making sure doesn t escape glass\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "is soloar radaition goes atmosphere turns heat\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "space is glass green house sunlight goes space\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere absorbs sun s heat keeps greenhouse\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere acts cover protect earth sunlight allowing light come glass ceiling acts cover\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "glass atmosphere traps heat energy\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "keeps\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "picture b is green house is similar suns raise bouncing earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "green room is supposed let suns raise come keep green room hot\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "heat sun passes atmosphere het sun going greenhouse be passing glass panels lets heat keeps heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "atmosphere traps heat radiation sun glass does\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere is glass greenhouse let radiation trap heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i think is acts glass heat go be relseted\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere is glass greenhouse heat come get bounces atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere let light trap things infrared radiation\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "glass atmosphere keep heat don t keep infrared radiation\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "heat is trapped atmosphere greenhouse gases reflects infrared light earth surface\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "is atmosphere lets heat traps heat does come reflects heat glass greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere acts glass geenhouse suns radiation goes\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "allows sunlight enter earth glass allows sunlight enter greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "does same job green house gases\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere surrounds earth keeps hot air is glass greenhouse keeps hot air\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "suns heat is coming greenhouse sun heat is coming earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere is bubble surrounding earth acts glass greenhouse acts contain heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere traps radiation earth glass does same greenhouse surfaces are penetrable able allow radiation are able retain heat\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "i think heat get trapped reflect plants\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere is glass s boundary holds heat solar oven light passes glass atmosphere traps\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "glass is light goes stay heat works same way atmosphere light hits\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "is green house gasses keep heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "s barrier heat stay\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere holds heat does bounce\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere traps reflect energy given sun\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere traps sun heats greenhouse is cover sun heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "glass blocks heat going green house atmosphere does\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "green are sister brothers\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "atmosphere lets radiation makes difficult heat radiation escape acts sort blanket atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "glass atmosphere keep heat energy escaping\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "sun radiation sun is entering\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i think sun is greenhouse heat sun goes glass\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "eh idk\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "atmosphere keeps heat glass keeps heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "allows light come earth be transfered light\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "s allowing heat go project heat earth\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "atmosphere is cleary stopping heat leaving arrows don t move atmosphere\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "light passes atmosphere heats earth heat leave earth much affects earth\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "is sun sun goes green house heats inside green house\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "carbon other greenhouse gases atmosphere act plexiglas letting light light\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere is sun s rays pass reach earth s surface atmosphere is glass greenhouse trap heat energy escaping\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "atmosphere is glass green house traps air\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "traps\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "i think glass greenhouse is atmosphere keeps air keeps suns reflections killing suns reflection shines greenhouse glass keeps heat atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere atmosphere earth protects allows certain amount sunlight radiation caused sun earth\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i chose heat tries comeout atmosphere reflects earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i chose atmosphere is surrounding is earth picture is glass heat energy is coming was space heat energy escape sun is circling earth is place big circle surrounding t be earth is is thing surrounding greenhouse heat energy\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think kind protection dome stays earth is window way is window be meteor coming towards earth atmosphere protect rainy day windows keep rain house\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "atmosphere reflects takes light energy same earth s atmosphere is doing diagrams\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere is greenhouse glass sunlight goes glass sunlight goes atmosphere earth t go atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "keeps solar radiation earth glass\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i said glass green house looks earth sun s heat goes earth does come\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere is glass green house sun rays travel become heat rise air t escape atmosphere is holding\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "glass lets light does let heat escape is atmosphere does\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere lets energy tries go space little bit escape most bouncing earth atmosphere is greenhouse glass\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere is glass lets sun rays pass doesn t let glass does greenhouse\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "atmosphere keeps i lot heat light gets earth greenhouse less light heat escape\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere is glass greenhouse sun goes atmosphere heat is trapped\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "sun is going glass getting warm greenhouse give plants light\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "pictures radiation enters dosnt exit\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "solar radiation passes glass greenhouse becomes infrared radiation is similar atmosphere do\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "atmosphere is is keeping heat moist water world greenhouse\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "atmosphere is light sun passes atmosphere glass serves insulator keeps heat energy escaping infrared radiation is reflected earth\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "atmosphere stops heat escaping lets solar radiation come\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "atmosphere is leting light ad leting light works solar oven\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "atmosphere glass traps solar radiation comes sun makes room hotter\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i say atmosphere is layers earth keep earth glass atmosphere plants air stay greenhouse glass stops air leaving most air atmosphere keeps oxygen leaving atmosphere glass are same thing\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "is windo reflects lets light\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere is greenhouse glass keeps heat energy escaping\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "earths holds heat glass greenhouse\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "is plastik rap\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "glass green house allows solar radiation enter release infrared radiation capture heat energy\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere ozone layer reflect energy sun shoots earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "atmosphere trap heat earth green house gasses makes sun rays stuck atmosphere\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "earth absorbs sunlight atmosphere keeps leaving\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "heat is getting trapped air heating earth\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "is greenhouse glass heat is going is escaping\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "heat is going earth s atmosphere is being trapped\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "air is polluted keeps warm air cold air\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "atmosphere allows solar radiation enter prevents infrared radiation exit glass solar oven behaves same way\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "i picked glass lets heat won t let\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[2]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[1]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[2])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prediction probabilities to see confidence level of predictions and defer to manual grader if needed. Work with the prediction probability feature of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56343279607120411"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "               ('eclf2', pipeline2),\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'eclf2__transformer_weights_body_stats': (0.5, 0.75, 1.0),\n",
    "    'eclf2__transformer_key_words_dark': (0.5, 0.75, 1.0)\n",
    "}\n",
    "\n",
    "#parameters = {}\n",
    "\n",
    "grid_search = GridSearchCV( pipe3, parameters, n_jobs=-1, verbose=1, cv = 3)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe3.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_set[\"Answer\"], train_set.KIScore)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
