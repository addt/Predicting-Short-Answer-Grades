{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the pre and post test data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the '|' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, encoding = 'mbcs')\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['KIScore'] = df['KIScore'].astype(int)\n",
    "    df['Answer'] = df['Answer'].astype(str)\n",
    "    # Filters if needed later on\n",
    "    #filtered_data = df[\"Answer\"].notnull()\n",
    "    #filtered_data = df[df[\"KIScore\"] != 1 & df['Answer'].notnull() & df[\"KIScore\"].notnull()]\n",
    "    #df_narrative = df[filtered_data]\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda - Steve \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    #print the report on category breakdown, might need these counts later\n",
    "    #print(\"Creating training data... category breakdown:\")\n",
    "    #sorted_product_counts = df_narrative.Category.value_counts(ascending=True)\n",
    "    #print(sorted_product_counts)\n",
    "    #sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Categories\");\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into training and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set - Currently only genereates dev and test data\n",
    "#Modify the function later to keep some data as test data as well\n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    #randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['WISEID', 'Answer', 'KIScore']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    #separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    #get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"Laura_Question/Laura1.csv\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checker created by Peter Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    #print(re.findall('[a-z]+', text.lower()))\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_set['Answer'] = train_set['Answer'].apply(spell_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118478.0</td>\n",
       "      <td>Darker colored cars absorb the suns radiation.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154123.0</td>\n",
       "      <td>because light color dont drag heat.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154199.0</td>\n",
       "      <td>I chose light colored fabric inside the car be...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150071.0</td>\n",
       "      <td>Dark colors do not take the light as the green...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153926.0</td>\n",
       "      <td>Dark colors attract the sun so painting the ca...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  118478.0     Darker colored cars absorb the suns radiation.        4\n",
       "1  154123.0                because light color dont drag heat.        2\n",
       "2  154199.0  I chose light colored fabric inside the car be...        2\n",
       "3  150071.0  Dark colors do not take the light as the green...        2\n",
       "4  153926.0  Dark colors attract the sun so painting the ca...        2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dev_set['Answer'] = dev_set['Answer'].apply(spell_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118476.0</td>\n",
       "      <td>I choose use a light colored fabric on the ins...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150168.0</td>\n",
       "      <td>Why I chose the light colored answers is becau...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139815.0</td>\n",
       "      <td>Light colors, do not absorb heat as easily as ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150145.0</td>\n",
       "      <td>The paint will block of the sun light that wan...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153913.0</td>\n",
       "      <td>cause if its dark it might make it hotter than...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  118476.0  I choose use a light colored fabric on the ins...        2\n",
       "1  150168.0  Why I chose the light colored answers is becau...        3\n",
       "2  139815.0  Light colors, do not absorb heat as easily as ...        3\n",
       "3  150145.0  The paint will block of the sun light that wan...        3\n",
       "4  153913.0  cause if its dark it might make it hotter than...        2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies that need to be attempted </b>\n",
    "1. Feature Selection attempted:\n",
    "    1. counts of Unigrams only \n",
    "    2. ... Unigrams and Bigrams\n",
    "    3. ... Unigrams, Bigrams, and Trigrams\n",
    "    4. ... Bigrams and Trigrams\n",
    "    5. ... 4- and 5-gram combinations\n",
    "    6. The use of TF-IDF, with IDF and without\n",
    "    7. Word tokens that included punctuation and numbers\n",
    "    8. Word tokens with letters only, filtering punctuation or splitting on punctuation\n",
    "    9. Lemmatizing using Word Net\n",
    "    10. Stemming using Snowball\n",
    "    11. With and without stopwords\n",
    "    12. With and without lowercasing\n",
    "    13. Chunking out all words that are not nouns.\n",
    "    14. Stemming user Porter and Lancaster stemmers.\n",
    "    15. Checking most common hypernyms of nouns in the review to categorise reviews better.\n",
    "    16. Using feature unions in pipelines to select specific features.\n",
    "2. Classifiers used:\n",
    "    1. Linear: Naive Bayes, Linear Regression, Stochastic Gradiant Descent\n",
    "    2. SVC and Linear SVC (One vs One, One vs Many)\n",
    "    3. K - Nearest Neighbor\n",
    "    4. MLP\n",
    "    5. Voting classifiers with hard and soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Answer\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Answer\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple Naive Bayes classifier for Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB_model(train_set, arr_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train, arr_dev = transform_dfs_to_arrays(train_set, dev_set)\n",
    "nb_model = train_NB_model(train_set, arr_train)\n",
    "nb_predictions = nb_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67292225201072386"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNearest_model(train_set, arr_train):\n",
    "    #Should add and experiement with more parameters and algorithms for nearest neighbor\n",
    "    nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53351206434316356"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_model = train_KNearest_model(train_set, arr_train)\n",
    "ne_predictions = neigh_model.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, ne_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad for a start, lets move onto Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LR_model(train_set, arr_train):\n",
    "    logreg = LogisticRegression()\n",
    "    lr_model = logreg.fit(arr_train, train_set.KIScore)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = train_LR_model(train_set, arr_train)\n",
    "lr_predictions = lr_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69168900804289546"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already nearing 80s!!!! But remember, need to measure Cohen's Kappa, not percetage correct. Also start plotting confusion matrix and extract errors once the classifiers are worked out.\n",
    "\n",
    "Lets start with the pipeline for the best features and get to feature detections using SVM. Also need to perform all the combinations mentioned before (Including preprocessing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    #this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline attempts - Best features will be decided using Grid Search. Lets just setup a baseline for now.\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#Note: add probability True to SVC classifier to be able to use predict probability function, which\n",
    "# is crucial for the the ensemble methods tried later\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\")),\n",
    "                      ('tfidf', TfidfTransformer(use_idf = True, norm='l2')),\n",
    "                      ('log', LogisticRegression(class_weight = None )),\n",
    "                      ('clf', SVC(C = 1000000.0, gamma='auto', kernel='linear', probability = True))])\n",
    "                      #('clf', LinearSVC(C=1.0, random_state=69, penalty='l2', dual=True, tol=1e-5, class_weight = None))])\n",
    "                      #('clf', OneVsOneClassifier(LinearSVC(random_state=0)))])                    \n",
    "                      #('clf', SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64343163538873993"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predictor = text_clf.fit(train_set[\"Answer\"], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = pipeline_predictor.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>143</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>21</td>\n",
       "      <td>191</td>\n",
       "      <td>86</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1    2   3   4   5  All\n",
       "True                               \n",
       "1          14    0   0   0   0   14\n",
       "2           6  143  24   1   0  174\n",
       "3           1   40  47  19   1  108\n",
       "4           0    8  11  32   8   59\n",
       "5           0    0   4  10   4   18\n",
       "All        21  191  86  62  13  373"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some ensemble classifiers (Both averaging and boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70509383378016088"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f_clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C = 100, penalty=\"l1\", dual = False))),\n",
    "  ('classification', forest_clf)\n",
    "])\n",
    "\n",
    "forest_predictor = f_clf.fit(arr_train, train_set.KIScore)\n",
    "f_predicted = forest_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, f_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55227882037533516"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "ada_predictor = ada_clf.fit(arr_train, train_set.KIScore)\n",
    "a_predicted = ada_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, a_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65683646112600536"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0).fit(arr_train, train_set.KIScore)\n",
    "grad_predictor = grad_clf.fit(arr_train, train_set.KIScore)\n",
    "grad_predicted = grad_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, grad_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And moving on to voting classifier with hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.13, NNZs: 5000, Bias: 1.383847, T: 1494, Avg. loss: 0.490720\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.24, NNZs: 5000, Bias: 1.643781, T: 2988, Avg. loss: 0.364141\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.34, NNZs: 5000, Bias: 1.761446, T: 4482, Avg. loss: 0.305853\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.43, NNZs: 5000, Bias: 1.845980, T: 5976, Avg. loss: 0.270908\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.49, NNZs: 5000, Bias: 1.873414, T: 7470, Avg. loss: 0.247234\n",
      "Total training time: 0.10 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.69436997319034854"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf9 = SGDRegressor(shuffle = True, verbose = 1)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf5', clf5), ('clf8', clf8),\n",
    "                                   ('clf9', clf9)], voting='hard')\n",
    "\n",
    "eclf_predictor = eclf.fit(arr_train, train_set.KIScore)\n",
    "v_predicted = eclf_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, v_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SGD', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Voting classifier with soft voting\n",
    "\n",
    "Note: The MLP classifier improved accuracy for both hard and sofr voting\n",
    "\n",
    "But, I need to do a ton of cross validation for the correct parameters and classifiers for each question type. Not to mention, need to get the grid search working well for these things.\n",
    "\n",
    "TODO: Find optimal weights for the classifiers\n",
    "\n",
    "TODO: Need to do feature engineering to get better parameters. This isnt working too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_s = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], voting='soft')\n",
    "\n",
    "eclf_s_predictor = eclf_s.fit(arr_train, train_set.KIScore)\n",
    "s_predicted = eclf_s_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, s_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a brute force method to find the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w4', 'w5', 'w6', 'mean', 'std'))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "t0 = time()\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w4 in range(1,4):\n",
    "            for w5 in range(1,4):\n",
    "                for w6 in range(1,4):\n",
    "                        if len(set((w1,w2,w4,w5,w6))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        t0 = time()\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[w1, w2, w4, w5, w6], voting = 'soft')\n",
    "                        scores = cross_val_score(eclf, \n",
    "                                                 arr_train,\n",
    "                                                 train_set.KIScore,\n",
    "                                                 cv=3,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs= -1)\n",
    "                        \n",
    "                        print(\"done in %0.3fs\" % (time() - t0))\n",
    "                        df.loc[i] = [w1, w2, w4, w5, w6, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "                        \n",
    "#print(\"done in %0.3fs\" % (time() - t0))\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69436997319034854"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 3, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "eclf_w_predictor = eclf_w.fit(arr_train, train_set.KIScore)\n",
    "w_predicted = eclf_w_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, w_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! Not very good\n",
    "\n",
    "Lets see which categories we are getting wrong (Don't be 2!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(dev_set.KIScore, w_predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it looks like we should try and get more data. Might not be possible without mixing up student responses.\n",
    "\n",
    "To squeeze voting classifiers into the grid, I'm restricted to using only classifiers that provide a predict_pobability function. Might be worth trying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_grid = grid_search.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have exhausted all classifiers without really messing around with feature engineering or feature selection, lets add custom features to the pipeline using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Classifier for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "# Weights - 3 3 1 2 1\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),   \n",
    "    ('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "#p_predictor = pipeline.fit(train_set['Answer'], \n",
    "                                  #train_set.KIScore)\n",
    "\n",
    "#predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "#accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Dark(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('dark' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'black' in [ps.stem(i) for i in text.split()])\n",
    "                and 'absorb' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Keywords_Light(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Dark': ('light' in [ps.stem(i) for i in text.split()]\n",
    "                          or 'white' in [ps.stem(i) for i in text.split()])\n",
    "                and 'reflect' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]    \n",
    "    \n",
    "class Keywords_Albedo(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Albedo': 'albedo' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Trap': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_dark', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Dark()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_light', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Light()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),            \n",
    "        ('key_words_albedo', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Albedo()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1.0,        \n",
    "            #'key_words_dark': 1.0,\n",
    "            #'key_words_light': 1.0,\n",
    "            #'key_words_albedo': 1.0,        \n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()), \n",
    "    ('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 14   0   0   0   0]\n",
      " [  6 143  24   1   0]\n",
      " [  1  40  47  19   1]\n",
      " [  0   8  11  32   8]\n",
      " [  0   0   4  10   4]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEpCAYAAAD4Vxu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1afcbc99588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(dev_set.KIScore, predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light colors, do not absorb heat as easily as darker colors.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The paint will block of the sun light that wants to get out\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I chose using a light-colored paint on the outside, and a light-colored fabric on the inside because light colors reflect the light which makes it cooler, and dark colors absorb all the light, which makes it hotter.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "It would be best for her to use all light colors because the light will reflect off of it. If she use dark colors, her car would absorb all heat and it would be extremely hot inside and on the outside of her car.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "light colored paint reflect most of the electromagnetic waves, which mean it absorb less radiation from the sun. This mean it absorb less heat.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "a dark color like black attract more sumlight\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Using a light-colored paint on the outside and having light-colored fabric on the interior will keep the car cool while in the sun apposed to dark paint and dark fabric because dark colors attract the sun.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Sun/heat is attracted to dark colors.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the light color of the car will deflect some of the suns rays, and any other heat that manages to get in will be reflected by the light-colored seats.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "Using light colored outside and inside paint will make it cooler than having the dark paint absorbing sun light and the make the car hot.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The bright colors would reflect the sun's heat more, and the dark colors would absorb the heat so the car would be warmer.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "because the light color in side of the car to reflt the heat.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "Using light colored paint and fabric on the car will reflect radiation, preventing heat from being able to penetrate and collect in the car.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Light colored paint on the outside cause the color wont absorb that much heat than the darker colors.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I chose dark colored paint on the outside because radiation and heat have a harder time getting through darker things.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I think using light-colored fabric in the inside of the car is most important to keep the car not too hot because then it won't absorb a lot of heat and Laura will probably want the inside of the car to kep cool and will not care very much about the outside being hot. \n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "dark attracts heat and light colors reflect heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the light is being reflected by the light colored things in the car and around it sending the light into the air making it hotter not the car\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "It will not take in any heat.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "the dark color does not heat the car\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "because the light fabric will protect the car.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "The light color on the outside of the car helps reflect light energy. The light color fabric in the inside also does the same but it reflects the energy from the inside of the car.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "In order to keep the car nice and cool while it sits in the sun, using a light color would be the best choice because light colors tend to absorb less heat and reflect all wavelengths of light. \n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I picked light colored paint and fabric because the sun reflects off of light colors and darker colors attract the heat.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Light colors prevent heat from going inside and creating conduction. Dark colors take the heat and seal it so that it goes inside. So if your car is hot, it's probably because you have dark colors on the inside and/ or outside.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "Light colors reflect the sun's rays. Whereas dark colors absorb it.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "white things reflect the sun back up\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "it should be light colored because so the sun does not heat the car because when it's dark colored it it heats up . she should also have light colored fabric because it wont heat up as fast as dark colored fabric .\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Light colors reflect light, so the solar radiation reflects off the cars surface, preventing much of it from directly being absorbed into the car. \n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "light color does not absorb the sun dark colors do.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I think its important to have light paint on the outside, because the it will make it reflect off the car. If it were dark paint it would attract the sun instead of reflect it.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Dark fabric obsorbs the sun and light fabric doesnt.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the dark colored paint will act like shade \n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "It should be a light color outside the car, like white, because light colors don't absorb the heat like dark colors do.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I picked these choices because, dark colors like black are known to absorb heat. Light colors like white are known to reflect light. This explains my choices.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "Heat soaks into darker colors.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Because using a light-colored paint on the outside helps the car to reflect the sun's heat, therefore it won't be absorbed.a\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Dark colors attract more heat then lighter colors so if you don't want your car to get hot use warmer colors.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "using light colored fabric will not absorb the heat such as the dark colored fabric that will absorb the heat.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I chose A and C because they both represent light colors. I know that light colors reflect the sun and dark colors obsorb the sun.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Because light colors repel heat.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "I chose Using a light-colored paint on the outside\" because usually white colors do not attract the sun\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I chose to use a light colored aint and fabric because light colors like white reflects light. The car will be cool all day long. If she used dark colored fabric and paint they will absorb the liight and will make the car too hot.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "the liter it is the less the sun atrks\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Because if it's darker the fabric will absorb the light but if it's lighter it will deflect it.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "The light color fabric reflects the sun from getting hot.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "a dark color will obsorb the heat and the light color will reflect the heat there for you have a light color everything\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I chose this because dark colors trap heat while dark ones dont.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Light colors reflect light, while darker colors absorb light.\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "I think the outside and inside of the car should be light colored because the dark colors really absorb a lot of solar energy and light colors do not.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i think your car should be a light colored paint because the sun makes the sun hot.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "why is that the car yes needs a different color but i don't think that's the main reason why it's so hot beacuse when my dad's car sits in the sun its bright color green but when we get in it it's really hot beacuse the seats are dark brown so it would contrap more heat from outside to inside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the darker the color the more heat it absorbs so the hotter it gets and the lighter the color the color it gets so the car wont get to hot on the inside if its a lighter color on the inside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "She should use a light colored fabric on the outside of the car because the light color will not hold in the heat.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "dark colors atract more haet\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "I think that she should use light-colored paint on the inside and outside because if you use dark colored paint then the it will get hot because dark always makes it hot.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "I chose those answers because the sun will go to darker colors mostly black. But using light colors it will prevent this from happening.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Black absorbs more heat from light.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Because if u use it in the outside it could fall or it could be stolen. And also becaue if you use a dark color it atracts the sun even more and make the car more hot and plus if you use a light color it bounces off.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I said to have a light fabric on the inside and a light paint on the outside because dark colors attract the heat and the light colors deflect the heat.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The light color (lets say white) will reflect the heat rays from the sun.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I chose using using dark colored fabric inside the car because if you were a white colored shirt then you will get sunburn.The car will burn up if you put light colored fabric.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "I choose my answer because the dark fabric will shade out the sunlight\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the black thing orsevers the heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Because using a dark-colored paint or dark-colored fabric can make the car hotter because dark colors absorb more energy or heat.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "I think she should use light paint, and light material because dark atracts heat and light doesn't .\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "light color don't absorb as much heat as darker color do. So  if her car has lighter colored car it won't won't get as hot as a darker colored car.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I think light colored paint because if you use white then it will move away from the sun and if you use dark colors like black then that will attract the sun.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "light colors reflect other colors which makes lighter  colors cooler when in the sun.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I think she should use light colors because the sun reflex more to black.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Dark colored thing attrct sun more causing them to heat up quicker.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The lighter color will not absorb as much sun radiation then the black color  \n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "I think she should use a light colored fabric in the car because if you used a dark colored fabric in the car it will get hot because darker colors absorb more heat.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Because the sun reflects off bright colors.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Dark colored fabric absorbs heat while light colored fabric reflects heat.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "well darker things trap all the heat so you want lighter things if you want to keep your car cool.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because if we used dark colored fabric it will absorb light and make the car hotter\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "I choose my answer because heat bounces off the light colors instead of dark colors, because dark colors absorb heat.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "Darker colors will absorb heat, so if there is a lighter color fabric inside it will cool down faster.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "well when you have a lighter or white car the light reflects off of the white and plus she should put those aluminum covers  \n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "Light colors will absorb less radiation so it will be cooler even if it is sitting in the sun. \n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Laura should use light colored fabric for the inside and the outside of the car because light colors don't absorb heat as much as dark colors do.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "well,the darker the color is the more heat it obsorbs \n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the light will relflect the sun.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "My choices were the third and first one because if you have have any dark color on your car the sun will instantly hit your car because of the colors it has same goes to the fabric.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Light colors reflect light more than dark colors.\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "Her car has to be light because dark colors attract sunlight and she wants to keep her car cool.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "A lighter color inside the car would mean the inside would be reflecting more sun than its absorbing.\n",
      "Graded as: 5\n",
      "Actual grade is 3\n",
      "\n",
      "I did this because if it is hot light reflexs off of stuff!!!\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "most cars are dark colerd.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Cause we tried that.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "I chose light colored cloth because of the Alfredo light color gives.  \n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "White absorbs more color than black. Black does not reflect heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Using light colored fabric keeps the air cool and reflects the heat molecules.\n",
      "Graded as: 4\n",
      "Actual grade is 2\n",
      "\n",
      "A light colored paint job would reflect the sunlight meaning the car would not absorb the heat. \n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "I suggest to use a light- colored paint on the outside, because light colors usually absorb less heat than the dark colors. And I also suggest that use light-colored fabric on the inside of the car is because fabric is  a good insulator because the fabric has little fibers in it to keep the cool AIR in, which means that the fabric is strong.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "  I think that the inside of the car (the fabric) and the outside of the car should be light-colored because it doesn't atracts as much sun the dark colors.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "so the sun light will reflect.\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "sun shines on dark colored thing but reflects off light colored things.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Well, if you use light colored fabric or paint,the heat will reflect off the colors of heat. If you use dark colored fabric and paint, the the heat will store the energy from the sun which will make the car hot. \n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "The light paint and fabric will have the heat bounce off instead of being trapped.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "you should use light colors on and in the car because the sun aborbs the the dark colors such as black  if you have light colors the car will not get as hot that is why when you make solar ovens you use black paper\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The lighter the color is, the less the sun gets attracted too. So when you have a dark shirt and your walking outside you will feel much more heat than when your outside and wearing lighter color shirt.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "dark colors attract the lite but light colors do not attract sun light.\n",
      "That is why i chose to use light colors.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The inside of the car would get hotter if it had lighter colored paint, and fabric instead of dark paint and light fabric because, if the outside was wight and the inside was wight the sun wouldn't absorb the light but instead reflects, and that would get stick in the car, but black paint would absorb the light, and your furniture stays cold.  \n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "Light colors don't absorb as much energy from the sun as you can with dark colors.\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "Light colors doesn't attracted heat.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "By using light colored fabric on the inside of the car less sunlight will be absorbed and it will be reflected instead.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "If the colors are lighter than light will reflect and be less likely to be absorbed as heat\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "You have to use light colors because then the car wont heat up as much if it where to be a car with a light color paint & dark fabric or dark paint w/ light fabric.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Using light colored fabric on the inside of the car will help keep the car cool because it doesn't absorb as much heat as dark colored fabric does.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I think she should use light-colored paint and light-colored fabric because light-colored things reflect the sun.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "The heat will bounce off it and not get hot.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "If you have lighter colors, the less light will be absorbed, making it cooler inside the car. \n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "If the fabric or the car is dark-colored the sun will be more attracted to the car and act as a solar oven and get hot.If the fabric or car is light-colored the sun will be less attracted to the car and it will become less hot.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Dark would not work because the sun is attracted to dark colors, not light colors.  So the sun will bounce off the light colors.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Dark colors attract the sun. So that means light colors don't.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Using almost anything light colored helps because light colors have a high albedo so the light reflects and goes somewhere else besides the car.  Dark colors have low albedos and so the heat would be absorbed,  making the car hotter.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "Light colered things reflect most light.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "She should use a light color because dark colors absorbs more heat than lighter colors, they get the most heat. Light colors reflect heat from the sun, which is why light colored clothes help us cool when its hot/ sunny.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Light colors don't absorb much heat, unlike dark colors that do. The light colors will prevent the sun from heating up too much.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "Light colored paint on the outside of the car is important because it doe not take in a lot of temperature from the sun, reflecting heat back.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Dark pigment absorbs more sunlight so you will feel more heat. \n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "Using light colored paint  because dark colors absorb heat  but if it is lihgt it wont abosorb\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The white colored paint would reflect the heat but if the white colored paint was in the inside the hot air would go in the car reflects off and gets trapped by the window so then the hot air would heat the car. That is why i would put it on the outside.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "why its better for her to use light color paint on the out side and Using dark paint fabric helps the car not heat up.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Darker colors absorb more heat so by using a lighter colored paint and fabric which absorbs less heat, this will help keep the car cooler. \n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "i think this because the light color don't prevent heat and dark colors do. And i also think the light colored fabric will work also because it won't burn your behind.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Dark color attracts heat while light color does not so much.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Darker things absorb heat and energy. This makes it warmer. If you use lighter colors, like white, then it will reflect the heat and energy. This makes the car, in this situation, cooler. The light car and fabric will reflect heat and energy making the car cool on a hot day\n",
      "Graded as: 5\n",
      "Actual grade is 4\n",
      "\n",
      "That way, the light is reflected off the car rather than absorbing it, like a dark colored car would.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "light colors bounce the sun rays back towards the sun, dark colors on the other hand  attract the rays to the car.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "I chose those two answers because anything with a light color like white, reflects of the heat from the sun, so it doesn't get as hot as dark colored things.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[2]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[1]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[2])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prediction probabilities to see confidence level of predictions and defer to manual grader if needed. Work with the prediction probability feature of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46322805423009916"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "               ('eclf2', pipeline2),\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'eclf2__transformer_weights_body_stats': (0.5, 0.75, 1.0),\n",
    "    'eclf2__transformer_key_words_dark': (0.5, 0.75, 1.0)\n",
    "}\n",
    "\n",
    "#parameters = {}\n",
    "\n",
    "grid_search = GridSearchCV( pipe3, parameters, n_jobs=-1, verbose=1, cv = 3)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe3.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_set[\"Answer\"], train_set.KIScore)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
