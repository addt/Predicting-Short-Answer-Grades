{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses (Embedded responses)\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the embedded responses data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to create plot confusion matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to read in a file\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_excel(filename)\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['QuestionType'] = df['QuestionType'].astype(int)\n",
    "    #df = df[df['QuestionType'] == 1]\n",
    "    df['Score'] = df['new_Score3'].astype(int)\n",
    "    df['Text'] = df['Text'].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate the data into training and dev data. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set \n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    # randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['Text', 'Score']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    # separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    # get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"EmbeddedOvens2.xlsx\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell checker created by Peter Norvig ###\n",
    "*Not used in final algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    #print(re.findall('[a-z]+', text.lower()))\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizes text to be all lowercase\n",
    "normalizer = lambda x : ' '.join(i for i in list(map(correct, tokens_target(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and/or Spellcheck Training and Dev Sets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_set['Text'] = train_set['Text'].apply(spell_checker)\n",
    "train_set['Text'] = train_set['Text'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oven works putting plexiglas top box putting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>final solar worked did add more reflection mat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are first solar work champ second one was dog ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score\n",
       "0  oven works putting plexiglas top box putting t...      1\n",
       "1  final solar worked did add more reflection mat...      2\n",
       "2                                                nan      1\n",
       "3                                                nan      1\n",
       "4  are first solar work champ second one was dog ...      1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dev_set['Text'] = dev_set['Text'].apply(spell_checker)\n",
    "dev_set['Text'] = dev_set['Text'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>solar oven was using radiation light rays hitt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oven be wide short have aluminium bottom top c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heat project black paper foil melt food</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heat bones tin foil box</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>behave tin foilflap reflect more light box pla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score\n",
       "0  solar oven was using radiation light rays hitt...      1\n",
       "1  oven be wide short have aluminium bottom top c...      2\n",
       "2            heat project black paper foil melt food      1\n",
       "3                            heat bones tin foil box      2\n",
       "4  behave tin foilflap reflect more light box pla...      1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data to be classified ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transforms data frames to arrays \n",
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Text\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Text\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    # this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.636363636364\n",
      "Cohen's Kappa:  0.386046511628\n"
     ]
    }
   ],
   "source": [
    "# Custom Classifier\n",
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "# Weights - 3 3 1 2 1\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Attract(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Attract': 'attract' in [ps.stem(i) for i in text.split()]\n",
    "                or 'attrakt' in [ps.stem(i) for i in text.split()]\n",
    "                or 'atract' in [ps.stem(i) for i in text.split()]\n",
    "                or 'atrakt' in [ps.stem(i) for i in text.split()]\n",
    "                or 'attrackts' in [ps.stem(i) for i in text.split()]\n",
    "                or 'attracts' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 3, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_attract', Pipeline([ # Give low weight\n",
    "                    ('Attract', Keywords_Attract()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        \n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "            #'body_stats': 1,\n",
    "            #'key_words_radiate': 1.0,\n",
    "            #'key_words_trap': 1.0,\n",
    "            #'bag_of': 1.0        \n",
    "        #},\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),   \n",
    "    ('feature_selection', SelectFromModel(ExtraTreesClassifier(), prefit=False)),\n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline1.fit(train_set['Text'], \n",
    "                                  train_set.Score)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Text'].values)\n",
    "print(\"Accuracy: \",accuracy_score(dev_set.Score, predicted))\n",
    "print(\"Cohen's Kappa: \",cohen_kappa_score(dev_set.Score, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1   2  3  All\n",
       "True                     \n",
       "1          10   2  0   12\n",
       "2           9   9  0   18\n",
       "3           1   0  2    3\n",
       "All        20  11  2   33"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.Score, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  0.727272727273\n",
      "2 :  0.787878787879\n",
      "3 :  0.818181818182\n",
      "4 :  0.666666666667\n",
      "5 :  0.757575757576\n",
      "6 :  0.727272727273\n",
      "7 :  0.727272727273\n",
      "8 :  0.575757575758\n",
      "9 :  0.69696969697\n",
      "10 :  0.636363636364\n",
      "Average: 0.712121212121\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for checking final classifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Get sizes of training and test sets\n",
    "rows, columns = df.shape\n",
    "\n",
    "kf = KFold(rows, 10, shuffle=True)\n",
    "avg_accuracy = 0\n",
    "count = 1\n",
    "for train_indices, test_indices in kf:\n",
    "    train_set = df.loc[train_indices] \n",
    "    dev_set = df.loc[test_indices] \n",
    "    \n",
    "    pipeline1 = pipeline1.fit(train_set['Text'], \n",
    "                                  train_set.Score)\n",
    "    \n",
    "    trial_predictions = pipeline1.predict(dev_set[\"Text\"])\n",
    "    accuracy = accuracy_score(dev_set.Score, trial_predictions)\n",
    "    print(count,\": \",accuracy)\n",
    "    avg_accuracy += accuracy_score(dev_set.Score, trial_predictions)\n",
    "    count = count + 1\n",
    "print(\"Average:\",(avg_accuracy/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can put the thermometer in the sun instead of putting it in the shade. It will make it hotter if the heat goes to the thermometer.\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "Because of our lack of organization of materials on the bottom of our oven,\n",
      "\n",
      "The other group said we could improve by: evening out the placements of our tinfoil, and most importantly add a large amount of black paper inside of the oven attract heat inwards as well as outwards.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "1.) We think that if we add tin foil to the far side of the box inside wall and it will change the way radiation is radiated into the box and the black paper absorbing the heat energy coming of the tin foil.\n",
      "2.) We are adding in more tape so that our box is more secure and the items we have on our box helping to heat up the box are going to stay still and work.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Because... Then the radiation coming off the heat lamp is going to be better supervised and the heat energy will stay in the box and heat  up the box\n",
      "\n",
      "The other group said to change: The inside of Kai and my box should have more tin foil in strategic locations and have black paper on the close side of the box.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Because... we lost a lot of heat\n",
      "\n",
      "The other group said to change: the holes in the box\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Because...the heat escaped through the cardboard box\n",
      "\n",
      "The other group said to change: to put tin foil on the inside walls of the box\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "The other group said to change: The tinfoil because it reflects and dosent help that much.\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "Because they didn't use an interior we think they should make tin foil walls so the radiation will reflect around and conducts into the thermometer.\n",
      "\n",
      "\n",
      "We said the other group could change: The walls into tin foil so the radiation will reflect around\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Because we used we used black paper inside of the box it heated up the box\n",
      "\n",
      "We said the other group could change:putting the black paper inside of the box so then the box would heat up more\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "Because their group put the black paper inside the box and it was hotter.\n",
      "\n",
      "The other group said to change: that we should try to put black paper inside the box.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Because... it can rip easily.\n",
      "\n",
      "We told the other group to put Plexiglas instead of plastic wrap.\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "We said the other group could change: The Aluminum Flap to face the black paper.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "We could of taped the plastic wrap to the box more so less heat could be coming out, if it was\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "We could improve the heat by adding more aluminum foil. We could also use plastic wrap.\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "We would add more aluminium foil to reflect the foil towards the plastic wrap.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "we could put another aluminum flap to reflect the solar rays into the box where it is trapped by the plastic wrap.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "we could have had the alumnin foil flap better postioined. Also Maybe the plastic wrap could have been tighter.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "We heard something that helped us on designing our solar oven. I had the idea of making the box black with duct tape. Somebody told us that that was not a good idea because the heat would be trapped outside of the box and no heat would go inside the box. So, we left our box cardboard and didn't put anything on it.  \n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "We did not take ideas form others we look at their projects and had a idea closer to that. \n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "make the plastic titer \n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "We are going to put more tin foil on the bottom of the box because when he light hits the aluminum flap it will bounce of the wall that is covered in aluminum that will hit the food.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "we will buy black paper and more tin foil.  The black paper is to absorb the heat inside the oven. The tin foil is to make a larger flap to direct the rays from the sun to the oven. \n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "we are going to cut the other door and make it int a aluminum flap   \n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "The solar oven heats up by the heat going through the plastic on the box and gets trapped inside of the box.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "The sunlight will pass through the plastic wrap, since the black paper inside will absorb the heat and the tin foil flap will reflect the heat inside, heating the inside of  the solar oven.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Our  solar oven works like this. We had a wing shaped tin foil on the back of the box so the radiation from the sun would bounce of it to hit the plastic wrap which let the radiation in but kept it trapped inside it. We also used black paper so the radiation would have a much harder time to get out of the box soo it coud generate more heat while letting out less.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "we did a great job over all because our box got a lot SR and IR also heat.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Our oven has foil around the perimeter of the box inside, the light rays (sun rays) will reflect into the box which will create convection. The black paper at the bottom of our box will attract the radiation.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "we said they can put tin foil on the side to have the heat reflect on the thermometor.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Because the heat will bounce off the tin foil and go into the box.\n",
      "\n",
      "The other group said to get a flap on our box.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "We got the idea of adding the tape to the bottom to trap the heat energy in the box.\n",
      "\n",
      "The other group told us to put tape on the bottom of the box, and that_ how we got the idea to add a little more tape.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Because:cut the top down and put the paper on the bottom of the cardboard\n",
      "\n",
      "We said the other group could change:cut more than half way cut part a little and make it tall add more black piece of paper all around the bottom toilet tin foil\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "They suggested we use plastic wrap because it was a lot thinner.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Because  it will help heat up their box faster.\n",
      "\n",
      "We said the other group could add some tin foil to two of their sides because on the other sides they already have but two sides.\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Because... \n",
      "\n",
      "We said the other group could change: that they put black paper on the outside of the box. We said they should put the black paper on the inside of the box. We said this because it will absorb the heat and trap it in the box if it is inside. You cannot cook food on the outside of the oven.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "We can improve our solar oven by putting 1 piece of tin foil and 3 pieces of black construction paper. Our changes are gonna attract more heat.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "We said the other group could change the positioning of the the tim foil and the black paper.\n",
      "Graded as: 3\n",
      "Actual grade is 1\n",
      "\n",
      "Because... they need to have more aluminum on the flap to help reflect more heat into the box they should also have a long piece of black paper on the opposing side to help attract the reflected heat.\n",
      "\n",
      "We said the other group could change: more aluminum  on the aluminum  flap to have a long black piece of paper on the opposing side of the flap.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "Because...They told us to put aluminum on the sides because the black paper was not helping attrackt anything on the sides and the aluminum would help reflect.\n",
      "\n",
      "The other group said to change: The other grou[p told us to put aluminum on the sides.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "The light was our radiation, and the flap helped us by reflecting the heat into our solar oven, while the plexiglass keep the radiation in the solar oven.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Our solar oven works by atractting  heat through the black paper and reflecting the radiation of the tinfoil. the plastic warp will make a conduction between with the tinfoil too.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Our solar oven worked by having tin foil on the insides of the box so that the solar radiation can bounce off the tin foil and heat up the box.  It also had black paper on the bottom so that it helps heat up the box as well. Our oven had an aluminum flap which helped reflect the solar radiation into the box.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Because... \n",
      "We said the other group could change:We gave them the information of having as little air as possible to come out with the thermometer in because it will make the solar oven hotter.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "What we think that will improve our solar oven will have to be using more things like tin foil that will attracted heat to our solar oven. For example like a real oven it has metal inside of it to heat things faster.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "Because it will keep heat trapped inside and raise up the heat.\n",
      "\n",
      "The other group said to change that you should try plastic wrap and that will help try to keep heat inside and trapped heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Because...it will let more heat in\n",
      "\n",
      "We said the other group could change:the shape of the box\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "radiation=the light hitting the solar oven, reflection=By using foil reflecting the light.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "Our solar oven has tin foil on the inside to attract the sun's rays, because it is shiny and reflects on the sun.  We also added black paper on the outside because black attracts heat and radiation.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "All the tin foil reflect the heat to the bottom of the oven.\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "They said to use more paper \n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "Because black attracts heat.\n",
      "Graded as: 1\n",
      "Actual grade is 3\n",
      "\n",
      "Because our oven didn't absorb much heat\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Need to examine some of the incorrectly classified responses\n",
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[1]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[0]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[1])))\n",
    "        print()\n",
    "        #print(z)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
