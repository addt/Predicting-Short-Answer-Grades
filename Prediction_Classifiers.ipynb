{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the pre and post test data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the '|' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, encoding = 'mbcs')\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['KIScore'] = df['KIScore'].astype(int)\n",
    "    df['Answer'] = df['Answer'].astype(str)\n",
    "    # Filters if needed later on\n",
    "    #filtered_data = df[\"Answer\"].notnull()\n",
    "    #filtered_data = df[df[\"KIScore\"] != 1 & df['Answer'].notnull() & df[\"KIScore\"].notnull()]\n",
    "    #df_narrative = df[filtered_data]\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda - Steve \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    #print the report on category breakdown, might need these counts later\n",
    "    #print(\"Creating training data... category breakdown:\")\n",
    "    #sorted_product_counts = df_narrative.Category.value_counts(ascending=True)\n",
    "    #print(sorted_product_counts)\n",
    "    #sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Categories\");\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into training and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set - Currently only genereates dev and test data\n",
    "#Modify the function later to keep some data as test data as well\n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    #randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['WISEID', 'Answer', 'KIScore']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    #separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    #get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"Car_Question/Car1.csv\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checker created by Peter Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    #print(re.findall('[a-z]+', text.lower()))\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_set['Answer'] = train_set['Answer'].apply(spell_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150093.0</td>\n",
       "      <td>i think wamer because the car is outside in th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151191.0</td>\n",
       "      <td>the car is cooler than the air outside.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118562.0</td>\n",
       "      <td>it would be warmer than the outside air becaus...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150165.0</td>\n",
       "      <td>If the car has been sitting outside in the sun...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150114.0</td>\n",
       "      <td>becuose to mack it warm in side</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  150093.0  i think wamer because the car is outside in th...        2\n",
       "1  151191.0            the car is cooler than the air outside.        2\n",
       "2  118562.0  it would be warmer than the outside air becaus...        3\n",
       "3  150165.0  If the car has been sitting outside in the sun...        2\n",
       "4  150114.0                    becuose to mack it warm in side        2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dev_set['Answer'] = dev_set['Answer'].apply(spell_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150058.0</td>\n",
       "      <td>It warmer then the outside air becouse the col...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139785.0</td>\n",
       "      <td>The car is completely closed for a week and be...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136490.0</td>\n",
       "      <td>In a black car it will absorb the light and he...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150900.0</td>\n",
       "      <td>It actually depends if the person has the wind...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153982.0</td>\n",
       "      <td>When it is hot outside inside the car is way h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  150058.0  It warmer then the outside air becouse the col...        2\n",
       "1  139785.0  The car is completely closed for a week and be...        2\n",
       "2  136490.0  In a black car it will absorb the light and he...        5\n",
       "3  150900.0  It actually depends if the person has the wind...        2\n",
       "4  153982.0  When it is hot outside inside the car is way h...        2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies that need to be attempted </b>\n",
    "1. Feature Selection attempted:\n",
    "    1. counts of Unigrams only \n",
    "    2. ... Unigrams and Bigrams\n",
    "    3. ... Unigrams, Bigrams, and Trigrams\n",
    "    4. ... Bigrams and Trigrams\n",
    "    5. ... 4- and 5-gram combinations\n",
    "    6. The use of TF-IDF, with IDF and without\n",
    "    7. Word tokens that included punctuation and numbers\n",
    "    8. Word tokens with letters only, filtering punctuation or splitting on punctuation\n",
    "    9. Lemmatizing using Word Net\n",
    "    10. Stemming using Snowball\n",
    "    11. With and without stopwords\n",
    "    12. With and without lowercasing\n",
    "    13. Chunking out all words that are not nouns.\n",
    "    14. Stemming user Porter and Lancaster stemmers.\n",
    "    15. Checking most common hypernyms of nouns in the review to categorise reviews better.\n",
    "    16. Using feature unions in pipelines to select specific features.\n",
    "2. Classifiers used:\n",
    "    1. Linear: Naive Bayes, Linear Regression, Stochastic Gradiant Descent\n",
    "    2. SVC and Linear SVC (One vs One, One vs Many)\n",
    "    3. K - Nearest Neighbor\n",
    "    4. MLP\n",
    "    5. Voting classifiers with hard and soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Answer\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Answer\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple Naive Bayes classifier for Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB_model(train_set, arr_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train, arr_dev = transform_dfs_to_arrays(train_set, dev_set)\n",
    "nb_model = train_NB_model(train_set, arr_train)\n",
    "nb_predictions = nb_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79951100244498774"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNearest_model(train_set, arr_train):\n",
    "    #Should add and experiement with more parameters and algorithms for nearest neighbor\n",
    "    nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71149144254278729"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_model = train_KNearest_model(train_set, arr_train)\n",
    "ne_predictions = neigh_model.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, ne_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad for a start, lets move onto Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LR_model(train_set, arr_train):\n",
    "    logreg = LogisticRegression()\n",
    "    lr_model = logreg.fit(arr_train, train_set.KIScore)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = train_LR_model(train_set, arr_train)\n",
    "lr_predictions = lr_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82396088019559899"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already nearing 80s!!!! But remember, need to measure Cohen's Kappa, not percetage correct. Also start plotting confusion matrix and extract errors once the classifiers are worked out.\n",
    "\n",
    "Lets start with the pipeline for the best features and get to feature detections using SVM. Also need to perform all the combinations mentioned before (Including preprocessing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    #this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline attempts - Best features will be decided using Grid Search. Lets just setup a baseline for now.\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#Note: add probability True to SVC classifier to be able to use predict probability function, which\n",
    "# is crucial for the the ensemble methods tried later\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\")),\n",
    "                      ('tfidf', TfidfTransformer(use_idf = True, norm='l2')),\n",
    "                      ('log', LogisticRegression(class_weight = None )),\n",
    "                      ('clf', SVC(C = 1000000.0, gamma='auto', kernel='linear', probability = True))])\n",
    "                      #('clf', LinearSVC(C=1.0, random_state=69, penalty='l2', dual=True, tol=1e-5, class_weight = None))])\n",
    "                      #('clf', OneVsOneClassifier(LinearSVC(random_state=0)))])                    \n",
    "                      #('clf', SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.80929095354523228"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predictor = text_clf.fit(train_set[\"Answer\"], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = pipeline_predictor.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>278</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>17</td>\n",
       "      <td>302</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1    2   3   4  5  All\n",
       "True                              \n",
       "1          12    2   0   0  0   14\n",
       "2           5  278  24   1  0  308\n",
       "3           0   19  34   4  0   57\n",
       "4           0    3  13   6  0   22\n",
       "5           0    0   3   4  1    8\n",
       "All        17  302  74  15  1  409"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some ensemble classifiers (Both averaging and boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82885085574572126"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f_clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C = 100, penalty=\"l1\", dual = False))),\n",
    "  ('classification', forest_clf)\n",
    "])\n",
    "\n",
    "forest_predictor = f_clf.fit(arr_train, train_set.KIScore)\n",
    "f_predicted = forest_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, f_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77995110024449876"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "ada_predictor = ada_clf.fit(arr_train, train_set.KIScore)\n",
    "a_predicted = ada_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, a_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80440097799511001"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0).fit(arr_train, train_set.KIScore)\n",
    "grad_predictor = grad_clf.fit(arr_train, train_set.KIScore)\n",
    "grad_predicted = grad_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, grad_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And moving on to voting classifier with hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82640586797066018"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf5', clf5), ('clf8', clf8)], voting='hard')\n",
    "\n",
    "eclf_predictor = eclf.fit(arr_train, train_set.KIScore)\n",
    "v_predicted = eclf_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, v_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SGD', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Voting classifier with soft voting\n",
    "\n",
    "Note: The MLP classifier improved accuracy for both hard and sofr voting\n",
    "\n",
    "But, I need to do a ton of cross validation for the correct parameters and classifiers for each question type. Not to mention, need to get the grid search working well for these things.\n",
    "\n",
    "TODO: Find optimal weights for the classifiers\n",
    "\n",
    "TODO: Need to do feature engineering to get better parameters. This isnt working too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_s = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], voting='soft')\n",
    "\n",
    "eclf_s_predictor = eclf_s.fit(arr_train, train_set.KIScore)\n",
    "s_predicted = eclf_s_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, s_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a brute force method to find the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 109.959s\n",
      "done in 112.551s\n",
      "done in 109.979s\n",
      "done in 110.940s\n",
      "done in 110.325s\n",
      "done in 111.450s\n",
      "done in 110.288s\n",
      "done in 112.476s\n",
      "done in 111.414s\n",
      "done in 109.858s\n",
      "done in 110.469s\n",
      "done in 110.105s\n",
      "done in 110.260s\n",
      "done in 112.139s\n",
      "done in 110.405s\n",
      "done in 109.968s\n",
      "done in 110.427s\n",
      "done in 110.279s\n",
      "done in 111.278s\n",
      "done in 109.543s\n",
      "done in 110.842s\n",
      "done in 110.845s\n",
      "done in 110.397s\n",
      "done in 111.184s\n",
      "done in 110.564s\n",
      "done in 110.407s\n",
      "done in 110.773s\n",
      "done in 111.640s\n",
      "done in 111.556s\n",
      "done in 110.569s\n",
      "done in 110.662s\n",
      "done in 111.503s\n",
      "done in 109.722s\n",
      "done in 109.249s\n",
      "done in 111.455s\n",
      "done in 111.114s\n",
      "done in 110.738s\n",
      "done in 110.282s\n",
      "done in 110.194s\n",
      "done in 112.136s\n",
      "done in 110.880s\n",
      "done in 109.975s\n",
      "done in 108.855s\n",
      "done in 111.330s\n",
      "done in 112.035s\n",
      "done in 110.776s\n",
      "done in 111.422s\n",
      "done in 110.167s\n",
      "done in 110.457s\n",
      "done in 109.818s\n",
      "done in 109.679s\n",
      "done in 110.885s\n",
      "done in 109.793s\n",
      "done in 109.957s\n",
      "done in 110.148s\n",
      "done in 109.162s\n",
      "done in 109.637s\n",
      "done in 111.927s\n",
      "done in 111.218s\n",
      "done in 109.040s\n",
      "done in 110.506s\n",
      "done in 110.526s\n",
      "done in 110.205s\n",
      "done in 111.533s\n",
      "done in 110.545s\n",
      "done in 109.898s\n",
      "done in 111.323s\n",
      "done in 111.993s\n",
      "done in 110.075s\n",
      "done in 111.588s\n",
      "done in 110.395s\n",
      "done in 111.739s\n",
      "done in 111.747s\n",
      "done in 110.508s\n",
      "done in 110.084s\n",
      "done in 110.625s\n",
      "done in 114.119s\n",
      "done in 111.884s\n",
      "done in 111.339s\n",
      "done in 110.358s\n",
      "done in 110.725s\n",
      "done in 110.979s\n",
      "done in 109.825s\n",
      "done in 111.332s\n",
      "done in 110.706s\n",
      "done in 111.352s\n",
      "done in 110.435s\n",
      "done in 110.104s\n",
      "done in 109.385s\n",
      "done in 110.966s\n",
      "done in 109.427s\n",
      "done in 110.904s\n",
      "done in 110.966s\n",
      "done in 111.183s\n",
      "done in 110.261s\n",
      "done in 109.354s\n",
      "done in 110.546s\n",
      "done in 109.574s\n",
      "done in 109.532s\n",
      "done in 109.702s\n",
      "done in 111.417s\n",
      "done in 110.973s\n",
      "done in 111.138s\n",
      "done in 112.092s\n",
      "done in 109.593s\n",
      "done in 111.454s\n",
      "done in 110.574s\n",
      "done in 109.524s\n",
      "done in 110.142s\n",
      "done in 109.980s\n",
      "done in 109.776s\n",
      "done in 109.926s\n",
      "done in 111.364s\n",
      "done in 109.732s\n",
      "done in 111.377s\n",
      "done in 110.417s\n",
      "done in 115.630s\n",
      "done in 111.803s\n",
      "done in 110.548s\n",
      "done in 110.903s\n",
      "done in 110.070s\n",
      "done in 110.302s\n",
      "done in 110.812s\n",
      "done in 110.751s\n",
      "done in 110.505s\n",
      "done in 110.149s\n",
      "done in 109.658s\n",
      "done in 111.067s\n",
      "done in 111.025s\n",
      "done in 111.466s\n",
      "done in 110.533s\n",
      "done in 110.073s\n",
      "done in 111.272s\n",
      "done in 109.042s\n",
      "done in 110.527s\n",
      "done in 110.390s\n",
      "done in 110.715s\n",
      "done in 110.329s\n",
      "done in 111.408s\n",
      "done in 110.731s\n",
      "done in 110.515s\n",
      "done in 110.209s\n",
      "done in 111.310s\n",
      "done in 110.466s\n",
      "done in 110.264s\n",
      "done in 110.666s\n",
      "done in 110.159s\n",
      "done in 111.799s\n",
      "done in 110.543s\n",
      "done in 111.509s\n",
      "done in 110.144s\n",
      "done in 111.241s\n",
      "done in 110.807s\n",
      "done in 110.867s\n",
      "done in 110.473s\n",
      "done in 111.343s\n",
      "done in 111.384s\n",
      "done in 110.381s\n",
      "done in 109.924s\n",
      "done in 109.464s\n",
      "done in 110.006s\n",
      "done in 110.986s\n",
      "done in 110.677s\n",
      "done in 111.201s\n",
      "done in 110.626s\n",
      "done in 110.633s\n",
      "done in 111.013s\n",
      "done in 110.612s\n",
      "done in 110.238s\n",
      "done in 109.979s\n",
      "done in 109.626s\n",
      "done in 111.084s\n",
      "done in 110.555s\n",
      "done in 110.928s\n",
      "done in 111.283s\n",
      "done in 110.108s\n",
      "done in 109.177s\n",
      "done in 110.082s\n",
      "done in 110.718s\n",
      "done in 110.660s\n",
      "done in 110.695s\n",
      "done in 110.399s\n",
      "done in 109.470s\n",
      "done in 109.401s\n",
      "done in 111.437s\n",
      "done in 109.650s\n",
      "done in 110.938s\n",
      "done in 110.433s\n",
      "done in 110.554s\n",
      "done in 110.909s\n",
      "done in 110.962s\n",
      "done in 110.376s\n",
      "done in 110.896s\n",
      "done in 110.632s\n",
      "done in 111.853s\n",
      "done in 110.254s\n",
      "done in 110.390s\n",
      "done in 110.047s\n",
      "done in 111.622s\n",
      "done in 111.047s\n",
      "done in 112.321s\n",
      "done in 110.922s\n",
      "done in 110.293s\n",
      "done in 111.010s\n",
      "done in 110.830s\n",
      "done in 110.787s\n",
      "done in 111.359s\n",
      "done in 111.704s\n",
      "done in 109.552s\n",
      "done in 111.392s\n",
      "done in 109.774s\n",
      "done in 111.392s\n",
      "done in 110.521s\n",
      "done in 109.834s\n",
      "done in 111.493s\n",
      "done in 110.637s\n",
      "done in 109.222s\n",
      "done in 110.374s\n",
      "done in 111.063s\n",
      "done in 109.696s\n",
      "done in 110.340s\n",
      "done in 110.412s\n",
      "done in 110.692s\n",
      "done in 110.503s\n",
      "done in 111.413s\n",
      "done in 110.393s\n",
      "done in 111.071s\n",
      "done in 109.707s\n",
      "done in 111.030s\n",
      "done in 110.278s\n",
      "done in 110.315s\n",
      "done in 111.329s\n",
      "done in 117.519s\n",
      "done in 129.708s\n",
      "done in 129.763s\n",
      "done in 128.827s\n",
      "done in 121.809s\n",
      "done in 124.649s\n",
      "done in 122.702s\n",
      "done in 127.145s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:41: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w6</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.813463</td>\n",
       "      <td>0.002583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.812244</td>\n",
       "      <td>0.004289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.812235</td>\n",
       "      <td>0.003588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811023</td>\n",
       "      <td>0.004849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811016</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811016</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811007</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810405</td>\n",
       "      <td>0.002975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810403</td>\n",
       "      <td>0.002743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>0.001273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809795</td>\n",
       "      <td>0.004397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809786</td>\n",
       "      <td>0.003033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809786</td>\n",
       "      <td>0.003033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809188</td>\n",
       "      <td>0.004843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809182</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809175</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809175</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808572</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808570</td>\n",
       "      <td>0.004232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808565</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807963</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807961</td>\n",
       "      <td>0.003201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807956</td>\n",
       "      <td>0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.807956</td>\n",
       "      <td>0.002728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807954</td>\n",
       "      <td>0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807954</td>\n",
       "      <td>0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807360</td>\n",
       "      <td>0.006914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.807353</td>\n",
       "      <td>0.004838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807351</td>\n",
       "      <td>0.006515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792670</td>\n",
       "      <td>0.003388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.002666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.000621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.002671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.791449</td>\n",
       "      <td>0.003962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791447</td>\n",
       "      <td>0.008802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.790864</td>\n",
       "      <td>0.013844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.790831</td>\n",
       "      <td>0.004210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.790246</td>\n",
       "      <td>0.011019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.790226</td>\n",
       "      <td>0.004229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.789621</td>\n",
       "      <td>0.006129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.789614</td>\n",
       "      <td>0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.789589</td>\n",
       "      <td>0.004889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.789025</td>\n",
       "      <td>0.012463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.788987</td>\n",
       "      <td>0.001899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.006872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787806</td>\n",
       "      <td>0.015458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787799</td>\n",
       "      <td>0.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787779</td>\n",
       "      <td>0.007166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787156</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787143</td>\n",
       "      <td>0.004576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.785955</td>\n",
       "      <td>0.010551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.785362</td>\n",
       "      <td>0.015034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.784752</td>\n",
       "      <td>0.015586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.782306</td>\n",
       "      <td>0.014441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.781654</td>\n",
       "      <td>0.007827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.781044</td>\n",
       "      <td>0.008363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.778627</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.771879</td>\n",
       "      <td>0.004579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.763932</td>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      w1   w2   w4   w5   w6      mean       std\n",
       "217  3.0  3.0  1.0  2.0  1.0  0.813463  0.002583\n",
       "190  3.0  2.0  1.0  2.0  1.0  0.812244  0.004289\n",
       "211  3.0  2.0  3.0  3.0  1.0  0.812235  0.003588\n",
       "214  3.0  3.0  1.0  1.0  1.0  0.811023  0.004849\n",
       "145  2.0  3.0  2.0  2.0  1.0  0.811016  0.003563\n",
       "238  3.0  3.0  3.0  3.0  1.0  0.811016  0.003563\n",
       "229  3.0  3.0  2.0  3.0  1.0  0.811007  0.000566\n",
       "136  2.0  3.0  1.0  2.0  1.0  0.810405  0.002975\n",
       "157  2.0  3.0  3.0  3.0  1.0  0.810403  0.002743\n",
       "181  3.0  1.0  3.0  2.0  1.0  0.810400  0.001273\n",
       "74   1.0  3.0  3.0  2.0  1.0  0.809795  0.004397\n",
       "101  2.0  1.0  3.0  2.0  1.0  0.809786  0.003033\n",
       "172  3.0  1.0  2.0  2.0  1.0  0.809786  0.003033\n",
       "223  3.0  3.0  2.0  1.0  1.0  0.809191  0.005421\n",
       "187  3.0  2.0  1.0  1.0  1.0  0.809188  0.004843\n",
       "226  3.0  3.0  2.0  2.0  1.0  0.809182  0.002425\n",
       "199  3.0  2.0  2.0  2.0  1.0  0.809175  0.001310\n",
       "208  3.0  2.0  3.0  2.0  1.0  0.809175  0.001310\n",
       "196  3.0  2.0  2.0  1.0  1.0  0.808572  0.003199\n",
       "127  2.0  2.0  3.0  2.0  1.0  0.808570  0.004232\n",
       "184  3.0  1.0  3.0  3.0  1.0  0.808565  0.001269\n",
       "160  3.0  1.0  1.0  1.0  1.0  0.807963  0.005217\n",
       "119  2.0  2.0  2.0  2.0  1.0  0.807961  0.003201\n",
       "220  3.0  3.0  1.0  3.0  1.0  0.807956  0.001729\n",
       "239  3.0  3.0  3.0  3.0  2.0  0.807956  0.002728\n",
       "163  3.0  1.0  1.0  2.0  1.0  0.807954  0.001964\n",
       "175  3.0  1.0  2.0  3.0  1.0  0.807954  0.001964\n",
       "232  3.0  3.0  3.0  1.0  1.0  0.807360  0.006914\n",
       "230  3.0  3.0  2.0  3.0  2.0  0.807353  0.004838\n",
       "154  2.0  3.0  3.0  2.0  1.0  0.807351  0.006515\n",
       "..   ...  ...  ...  ...  ...       ...       ...\n",
       "138  2.0  3.0  1.0  2.0  3.0  0.792670  0.003388\n",
       "7    1.0  1.0  1.0  3.0  3.0  0.792659  0.002666\n",
       "85   2.0  1.0  1.0  2.0  3.0  0.792659  0.000621\n",
       "40   1.0  2.0  2.0  2.0  3.0  0.792659  0.002671\n",
       "27   1.0  2.0  1.0  1.0  2.0  0.791449  0.003962\n",
       "5    1.0  1.0  1.0  3.0  1.0  0.791447  0.008802\n",
       "180  3.0  1.0  3.0  1.0  3.0  0.790864  0.013844\n",
       "0    1.0  1.0  1.0  1.0  2.0  0.790831  0.004210\n",
       "103  2.0  1.0  3.0  2.0  3.0  0.790246  0.011019\n",
       "135  2.0  3.0  1.0  1.0  3.0  0.790226  0.004229\n",
       "25   1.0  1.0  3.0  3.0  3.0  0.789621  0.006129\n",
       "109  2.0  2.0  1.0  1.0  3.0  0.789614  0.003956\n",
       "31   1.0  2.0  1.0  2.0  3.0  0.789589  0.004889\n",
       "9    1.0  1.0  2.0  1.0  2.0  0.789025  0.012463\n",
       "58   1.0  3.0  1.0  2.0  3.0  0.788987  0.001899\n",
       "64   1.0  3.0  2.0  1.0  3.0  0.788395  0.006872\n",
       "91   2.0  1.0  2.0  1.0  3.0  0.787806  0.015458\n",
       "22   1.0  1.0  3.0  2.0  3.0  0.787799  0.011601\n",
       "37   1.0  2.0  2.0  1.0  3.0  0.787779  0.007166\n",
       "82   2.0  1.0  1.0  1.0  3.0  0.787156  0.001302\n",
       "4    1.0  1.0  1.0  2.0  3.0  0.787143  0.004576\n",
       "13   1.0  1.0  2.0  2.0  3.0  0.785955  0.010551\n",
       "46   1.0  2.0  3.0  1.0  3.0  0.785362  0.015034\n",
       "18   1.0  1.0  3.0  1.0  2.0  0.784752  0.015586\n",
       "100  2.0  1.0  3.0  1.0  3.0  0.782306  0.014441\n",
       "55   1.0  3.0  1.0  1.0  3.0  0.781654  0.007827\n",
       "28   1.0  2.0  1.0  1.0  3.0  0.781044  0.008363\n",
       "19   1.0  1.0  3.0  1.0  3.0  0.778627  0.011461\n",
       "10   1.0  1.0  2.0  1.0  3.0  0.771879  0.004579\n",
       "1    1.0  1.0  1.0  1.0  3.0  0.763932  0.006052\n",
       "\n",
       "[240 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w4', 'w5', 'w6', 'mean', 'std'))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "t0 = time()\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w4 in range(1,4):\n",
    "            for w5 in range(1,4):\n",
    "                for w6 in range(1,4):\n",
    "                        if len(set((w1,w2,w4,w5,w6))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        t0 = time()\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[w1, w2, w4, w5, w6], voting = 'soft')\n",
    "                        scores = cross_val_score(eclf, \n",
    "                                                 arr_train,\n",
    "                                                 train_set.KIScore,\n",
    "                                                 cv=3,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs= -1)\n",
    "                        \n",
    "                        print(\"done in %0.3fs\" % (time() - t0))\n",
    "                        df.loc[i] = [w1, w2, w4, w5, w6, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "                        \n",
    "#print(\"done in %0.3fs\" % (time() - t0))\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83374083129584353"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 3, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "eclf_w_predictor = eclf_w.fit(arr_train, train_set.KIScore)\n",
    "w_predicted = eclf_w_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, w_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! Not very good\n",
    "\n",
    "Lets see which categories we are getting wrong (Don't be 2!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-788ae8ebe01f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pd.crosstab(dev_set.KIScore, w_predicted, \n\u001b[0m\u001b[0;32m      2\u001b[0m            \u001b[0mrownames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predicted'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             margins=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, w_predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it looks like we should try and get more data. Might not be possible without mixing up student responses.\n",
    "\n",
    "To squeeze voting classifiers into the grid, I'm restricted to using only classifiers that provide a predict_pobability function. Might be worth trying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 20000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l1', 'l2'),\n",
    "    #'clf_kernel': ('rbf', 'linear'),\n",
    "    'clf__C': (1000, 10000)\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV( pipeline, parameters, n_jobs=-1, verbose=1, cv = 9)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_set[\"Answer\"], train_set.KIScore)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_grid = grid_search.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have exhausted all classifiers without really messing around with feature engineering or feature selection, lets add custom features to the pipeline using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Classifier for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83129584352078245"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]\n",
    "                or 'energi' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'body_stats': 1,\n",
    "            'key_words_radiate': 1.0,\n",
    "            'key_words_trap': 1.0,\n",
    "            'bag_of': 1.0        \n",
    "        },\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),    \n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 14   0   0   0   0]\n",
      " [  1 298   9   0   0]\n",
      " [  0  29  25   3   0]\n",
      " [  0   5  14   3   0]\n",
      " [  0   1   5   2   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEpCAYAAAD4Vxu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x246429d2f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(dev_set.KIScore, predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a black car it will absorb the light and heat and make the car hotter. In a white car it will get hot but not as much heat because white reflects the light and black absorbs the lights. The sun is the main source of the car heating up.The air in the car is trapped so the car gets hotter\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "The car contains heat more and if it was in the sun it would warm up.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "It warmer inside because the air has nowhere to go.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The solar radiation from the sun enters the car and becomes heat which is trapped by the glass, thus the inside will be warmer.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "The car is insulated it would work just like a solar oven. It would heat up from the sun assuming that the car has its windows closed so cold air couldn't come through.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The car is parked in the sun and closed so it will not leak heat.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The heat energy from the sun will trap itself inside the car and will not escape, heating it up. This is an example of radiation.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The car will heat up in the sun and the heat will get trapped in the car causing the car to be warmer.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "It is the same because the air outside is the main source of heat or light. It might be hot depending on how much the sun was beaming on the car.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "Once the heat gets in it get trapped in there, building up greater temperature inside than outside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I answered Warmer because the sun's radiation and is transferring heat to the car.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "If the heat will shine on the car and it will not be driven for a week, then it will rise because radiation rays will flow onto the car and it will make the car warmer than the air is outside of the car.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "I think it will be warmer than the outside air because the car that has been sitting in the sun, so because of radiation, heat will be transferred to the car. The cold air wont go to the car because cold is the absence of heat and only warm air moves to cold air in the car so the car must be warmer.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The air would be warmer in the car than the outside air because he left his car in the sun so the car was heating up when he wasn't using it.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Because the inside of the car would trap the sunlight in and warm up the inside.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I chose warmer than the outside air because the glass heats up and it makes the whole car hotter, even if there wasn't a lot of sunlight .\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "nan\n",
      "Graded as: 1\n",
      "Actual grade is 2\n",
      "\n",
      "because if the car is parked in the and it is not coverd with light colors the it will get hot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I chose the second one because there might have been hotter days and the radiation from the sun would heat the car up even when it is cold    .\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "while Akbar park outside in the sun, the ray from the sun turn into heat as it hit the car, due to the car structure, the heat couldn't really escape, so its a continuous building of heat inside his car and the car would be much warmer than the outside air \n",
      "Graded as: 2\n",
      "Actual grade is 5\n",
      "\n",
      "It is warmer inside a car than the outside air because heat is being trapped in the inside the car but infrared light is not let out. Hot air and Cold air is trapped inside in a process called convection. cold air is going down and hot air is rising.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "It will be warmer than the outside air, because the sun gives off solar radiation which turns it into heat energy, and the car keeps the heat inside of it, so, when its outside in the sun, the car will be hotter than the outside temperature.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "I chose this answer because the sun is warming up the air inside the car. It doesn't matter whether the air outside is cold.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the heat goes in to the car when its open and the heat gets trapped in the car and it stays hot inside.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The car is sitting in the sun so the heat from the sun will be transferred into the car and trapped inside.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Radiation from the sun is going into the car through the windows, making it warmer. But, the air outside is cold, making the car exactly the same temperature because the hot and cold are working against each other, so neither can win.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The car is warmer, because the interior is black or gray, so it absorbs all of the color, and makes the car warmer. \n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it is warmer because all of the heat from the sun got in the car and cannot get out\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The temperature of the air inside the car is not affected by the wind currents that happen outside the car. Convection occurs outside which reduces the temperature of air outside the car. However, the air inside the car is shut off from wind and left to absorb all the heat from the sun's radiation.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The temp inside the car will be warmer than the outside air because of the greenhouse effect. Since the glass of the car is transparent more energy is absorbed from the sun causing it to be warmer. \n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "If it's a cold day and the car hasn't been used for a week, during the night it would be really cold so for seven days if it's just sitting there it would be colder than outside. Just because there's a little sunlight it still can't heat it up that much.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I chose this because the outside would act as a energy consumer and consume energy so the inside of the car can be warmer than the outside.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "If the sun is shining for a week on a car, the heat will probably get trapped in the car. (it would be muggy or moist in\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The car is warmer because the windshield is clear and has allowed sunlight to go through into the car. The car then traps the heat inside the car, making it warmer than outside.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "the sun is beating down on the car and making the inside of the car hotter.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "When a car is left not running in the sun, the car absorbs the heat and traps it in. Even in just one day the car can reach temperatures of over 100 degrees. Furthermore, leaving it in the sun for seven days (a week) everyday will just add on to the heat making it hotter than the outside hair.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The heat that is inside the car is trapped inside, and most of it does not leave the car which would make the car warmer than the outside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The car has been parked outside for a week in the sun, so the sunlight has probably gone through the windows and the heat has been trapped in the car. It is cold outside, but the car has been parked in sunlight for so long, the inside of the car must me warmer than the outside.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "It is warmer than the outside because it uses conduction. The heat comes into the car and it gets hot. Since there are windows that are all the way up the heat will not evacuate the vehicle.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "It is warmer because all the radiation from the sun is inside the car because the heat got trraped in the car\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I think it would be warmer because it was parked in the sun and the heat from the sun would heat up he car so when you go into it it would be warmer then it was outside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The car is warmer because the car is under direct sunlight, so the car is receiving solar radiation that passes through its windows. Since the windows are glass, it provides great insulation so that the solar radiation that has been converted into heat can not escape. So overtime, the car will slowly heat up.\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "Based on experience, a car is hotter on the inside than it is outside on a hot day. The energy that enters the car from the glass will stay trapped in the car. The car should be warmer than the outside air.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The inside of the car will be warmer because the energy from the sun will become trapped inside the car due to the glass windows. \n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "I think it is warmer than the outside air because the car probably acts as a greenhouse which traps heat energy emitted from the sun. \n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "Since the heat was trapped inside the car, it will feel hotter inside than the air outside.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it keeps the heat in\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The heat from the sun has been trapped inside the car thus the air inside the car is warmer then than the air outside.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "I chose warmer than the outside air because the sun is going to the car and the air in the car can't go out so the air in the car will heat up and will make the car warmer.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The energy was stored from the weather when Akbar closed his door.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "The air outside is cold, yet the sun is heating it up with solar radiation. On days like these, the car is just the same temperature. The car usually is the same temp of cold days; on hot days though the car is waaaay hotter inside and out.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The sun is hitting the car which warms up the inside because the windows allow sunlight to pass through the car into the inside of it, causing the heat to stay inside the car. Also, if the car has black seats (not saying that it does) it will absorb the heat from the sunlight which is another way that the car could stay warmer than the outside air.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "It would be warmer than the outside because it would be heated up by being parked in the sun\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "I think that it will be warmer then the air outside because the air is trapped inside a hot car in a hot day and the sun will reflect strait down to the glass which will make the inside of the car hotter.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Even though it is cold outside, the light from the sun travels through the windows and the windshield and that converts into heat. Since the windows and doors and probably closed, much of the heat is retained, keeping the inside of the car warmer than the outside air.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "It is warmer because if it has not been driven for a week and the sun shines on it it is heating up the car.That would be if the week would be warm and sunny.The sun is parked in the sun meaning that the heat waves would go directly into the car.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Because the heat gets trapped in the car and stays like that until fresh outside air gets into he car.\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "Glass is an insulator, but sunlight and radiation can be passed through them, causing the car to heat up. The heat does not escape as easy as it gets in.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "i think that the car would be warmer inside because in a car there aren't much places to get cold air in but if the windshield is wide open and the sun is hitting it and not much air is coming in then the sun will heat it up because once the hot air comes in then most of it stays in. and if it stays in then when more hot air comes in it will get hotter and hotter.  \n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "Since it is parked in the sun it has kept the warmth from it inside the car despite the cold air outside and being parked there for a week.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The car got hotter from being in the sun for a week. The hot air inside has been trapped, but not able to escape. So, anytime heat gets in, it cannot get out again. Therefor the inside must be hotter. Although it is cold that day, it might have been hot the rest of the week.\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "The car retains some heat, since it is enclosed.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "It's warmer because it attracts heat so heat will be trapped inside. \n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "I  think the air inside the car is warmer than the outside air because the sun radiation went through the car's window. e radiation that went into the car go warmer and warmer making it hotter in the car.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "The car was hot because all the heat from the sun got in the car and cannot get out.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "The infrared radiation that has entered the car will be harder to escape. Once the infrared radiation is in the car, it will take a while to release.\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "The sunlight makes heat in the car and the heat cant get out of the car.\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "The heat from the outside is trapped inside the metal car.\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "It is warmer than the outside air because it has been parked in the sun for a week and untouched so the radiation from the sun is going into the car and is trapped in the car.\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[2]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[1]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[2])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prediction probabilities to see confidence level of predictions and defer to manual grader if needed. Work with the prediction probability feature of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
