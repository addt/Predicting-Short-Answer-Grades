{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically Scoring Student Responses\n",
    "\n",
    "Avi Dixit and Elizabeth McBride\n",
    "\n",
    "<b>Introduction </b> Notebook to upload the pre and post test data into pandas dataframes and apply classification algorithms to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports - Consolidated imports for all functions used (or will eventually be used) by the notebook\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [1, 2, 3, 4, 5]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the '|' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, encoding = 'mbcs')\n",
    "    \n",
    "    #Force KIScore to int, otherwise reverts to float. Same for Answer. Forcing NaN to unicode\n",
    "    df['KIScore'] = df['KIScore'].astype(int)\n",
    "    df['Answer'] = df['Answer'].astype(str)\n",
    "    # Filters if needed later on\n",
    "    #filtered_data = df[\"Answer\"].notnull()\n",
    "    #filtered_data = df[df[\"KIScore\"] != 1 & df['Answer'].notnull() & df[\"KIScore\"].notnull()]\n",
    "    #df_narrative = df[filtered_data]\n",
    "    return df\n",
    "\n",
    "#reads in the training data into a panda - Steve \n",
    "#(code based on ANLP Notebook Intro to Pandas by Marti Hearst and Andrea Gagliano)\n",
    "def read_training_data(filename):\n",
    "    df_narrative = read_file(filename)\n",
    "    #print the report on category breakdown, might need these counts later\n",
    "    #print(\"Creating training data... category breakdown:\")\n",
    "    #sorted_product_counts = df_narrative.Category.value_counts(ascending=True)\n",
    "    #print(sorted_product_counts)\n",
    "    #sorted_product_counts.plot(kind='barh', figsize=(8,6), title=\"Categories\");\n",
    "    return df_narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into training and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#breaks the panda into a training set and a dev set - Currently only genereates dev and test data\n",
    "#Modify the function later to keep some data as test data as well\n",
    "\n",
    "def get_train_and_dev_sets(full_data, percent_dev):\n",
    "    #randomize the indices\n",
    "    random_index = np.random.permutation(full_data.index)\n",
    "    full_data_shuffled = full_data.ix[random_index, ['WISEID', 'Answer', 'KIScore']]\n",
    "    full_data_shuffled.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #break down the counts for the shuffled data\n",
    "    rows, columns = full_data_shuffled.shape\n",
    "    train_size = round(rows*(1 - percent_dev))\n",
    "    dev_size   = round(rows*percent_dev)\n",
    "    \n",
    "    #separate the training data from the development data\n",
    "    train_data = full_data_shuffled.loc[:train_size]\n",
    "    dev_data = full_data_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "\n",
    "    return train_data, dev_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in the test file into a panda\n",
    "def read_test_data(filename):\n",
    "    #get the file\n",
    "    df = read_file(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the code that calls the above functions - puts the data into a data frame\n",
    "df = read_training_data(\"Car1.csv\")\n",
    "train_set, dev_set = get_train_and_dev_sets(df,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checker created by Peter Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = open('big.txt').read()\n",
    "\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    #print(re.findall('[a-z]+', text.lower()))\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokens_target(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    words = re.findall('[a-z]+', text.lower())\n",
    "    tagged_POS_sents = nltk.pos_tag(words) # tags sents\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    #print(tagged_POS_sents)\n",
    "    if (len(tagged_POS_sents) > 1):\n",
    "        normed_tagged_words = [word[0].lower() for word in tagged_POS_sents\n",
    "                              if (word[1].startswith('N') or word[1].startswith('J') or word[1].startswith('V'))]\n",
    "        return normed_tagged_words\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "WORDS = tokens(TEXT)\n",
    "\n",
    "COUNTS = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)\n",
    "\n",
    "# Show what happens in the case of ties\n",
    "def correct_under_hood (word):\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell_checker = lambda x : ' '.join(i for i in list(map(correct, tokens(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set['Answer'] = train_set['Answer'].apply(spell_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150089.0</td>\n",
       "      <td>i think it is going to be warmer in the car th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118595.0</td>\n",
       "      <td>it will be warmer because if it hasn t been us...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139510.0</td>\n",
       "      <td>some of the heat from the sunlight is wrapped ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154124.0</td>\n",
       "      <td>no matter if it is a hot day or cold day since...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150075.0</td>\n",
       "      <td>i chose this because the air transfers the hea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  150089.0  i think it is going to be warmer in the car th...        2\n",
       "1  118595.0  it will be warmer because if it hasn t been us...        2\n",
       "2  139510.0  some of the heat from the sunlight is wrapped ...        3\n",
       "3  154124.0  no matter if it is a hot day or cold day since...        4\n",
       "4  150075.0  i chose this because the air transfers the hea...        2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_set['Answer'] = dev_set['Answer'].apply(spell_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WISEID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>KIScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151199.0</td>\n",
       "      <td>the car temperature will be the same because w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150906.0</td>\n",
       "      <td>because the car would never be as cold as the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118437.0</td>\n",
       "      <td>since the car was left outside for a while the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151190.0</td>\n",
       "      <td>it will be the same because there is nothing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139507.0</td>\n",
       "      <td>usually when it is cold but the car has been s...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     WISEID                                             Answer  KIScore\n",
       "0  151199.0  the car temperature will be the same because w...        2\n",
       "1  150906.0  because the car would never be as cold as the ...        2\n",
       "2  118437.0  since the car was left outside for a while the...        2\n",
       "3  151190.0  it will be the same because there is nothing t...        2\n",
       "4  139507.0  usually when it is cold but the car has been s...        2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies that need to be attempted </b>\n",
    "1. Feature Selection attempted:\n",
    "    1. counts of Unigrams only \n",
    "    2. ... Unigrams and Bigrams\n",
    "    3. ... Unigrams, Bigrams, and Trigrams\n",
    "    4. ... Bigrams and Trigrams\n",
    "    5. ... 4- and 5-gram combinations\n",
    "    6. The use of TF-IDF, with IDF and without\n",
    "    7. Word tokens that included punctuation and numbers\n",
    "    8. Word tokens with letters only, filtering punctuation or splitting on punctuation\n",
    "    9. Lemmatizing using Word Net\n",
    "    10. Stemming using Snowball\n",
    "    11. With and without stopwords\n",
    "    12. With and without lowercasing\n",
    "    13. Chunking out all words that are not nouns.\n",
    "    14. Stemming user Porter and Lancaster stemmers.\n",
    "    15. Checking most common hypernyms of nouns in the review to categorise reviews better.\n",
    "    16. Using feature unions in pipelines to select specific features.\n",
    "2. Classifiers used:\n",
    "    1. Linear: Naive Bayes, Linear Regression, Stochastic Gradiant Descent\n",
    "    2. SVC and Linear SVC (One vs One, One vs Many)\n",
    "    3. K - Nearest Neighbor\n",
    "    4. MLP\n",
    "    5. Voting classifiers with hard and soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_dfs_to_arrays(train_set, dev_set):\n",
    "    vec = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", max_features=5000)\n",
    "    arr_train_feature_sparse = vec.fit_transform(train_set[\"Answer\"].values.astype(str))\n",
    "    arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "    \n",
    "    arr_dev_feature_sparse = vec.transform(dev_set[\"Answer\"].values.astype(str))\n",
    "    arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "        \n",
    "    return arr_train_feature, arr_dev_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple Naive Bayes classifier for Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB_model(train_set, arr_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train, arr_dev = transform_dfs_to_arrays(train_set, dev_set)\n",
    "nb_model = train_NB_model(train_set, arr_train)\n",
    "nb_predictions = nb_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74327628361858189"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNearest_model(train_set, arr_train):\n",
    "    #Should add and experiement with more parameters and algorithms for nearest neighbor\n",
    "    nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb_model = nb.fit(arr_train, \n",
    "                      train_set.KIScore)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67237163814180934"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_model = train_KNearest_model(train_set, arr_train)\n",
    "ne_predictions = neigh_model.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, ne_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not bad for a start, lets move onto Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LR_model(train_set, arr_train):\n",
    "    logreg = LogisticRegression()\n",
    "    lr_model = logreg.fit(arr_train, train_set.KIScore)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = train_LR_model(train_set, arr_train)\n",
    "lr_predictions = lr_model.predict(arr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78239608801955995"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already nearing 80s!!!! But remember, need to measure Cohen's Kappa, not percetage correct. Also start plotting confusion matrix and extract errors once the classifiers are worked out.\n",
    "\n",
    "Lets start with the pipeline for the best features and get to feature detections using SVM. Also need to perform all the combinations mentioned before (Including preprocessing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        #self.snow = SnowballStemmer('english')\n",
    "    \n",
    "    #this code will filter punctuation from a word and rejoin it together (\"they're\" becomes \"theyre\")\n",
    "    def __preprocess(self, doc):\n",
    "       filter_punc = lambda t: ''.join([x.lower() for x in t if x.isalpha()])\n",
    "       words = [x for x in map(filter_punc, doc.split()) if x]\n",
    "       review = \"\"\n",
    "       for w in words:\n",
    "           review = review+\" \"+w\n",
    "       return review\n",
    "    \n",
    "    #Multiple attempts to select lemmas and stems from a word token (using NLTK)\n",
    "    def __call__(self, doc):\n",
    "        #return [self.wnl.lemmatize(t.lower()) for t in word_tokenize(doc)]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [\"\".join([str(s.name()) for s in wn.synset(t).hypernyms()]) for t in word_tokenize(self.__preprocess(doc))]\n",
    "        #return [self.snow.stem(t) for t in word_tokenize(self.__preprocess(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "    \n",
    "def stuff(doc):\n",
    "    #flatten = [w for sent in doc for w in sent]\n",
    "    flatten = [w for w in word_tokenize(doc)]\n",
    "    unigram_counts = Counter(flatten)\n",
    "    uni_dist = FreqDist(unigram_counts)\n",
    "    uni = [a for (a, b) in uni_dist.most_common(25)]\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(doc) # Split text into sentences\n",
    "    words = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    tagged_POS_sents = [nltk.pos_tag(word) for word in words ] # tags sents\n",
    "    #print(tagged_POS_sents)\n",
    "    #normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           #for word in sent \n",
    "                           #if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           #and word[0] not in punctuation # remove punctuation\n",
    "                           #and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           #and word[1].startswith('N')]  # include only nouns\n",
    "    normed_tagged_words = [word[0].lower() for sent in tagged_POS_sents\n",
    "                          for word in sent\n",
    "                          if (word[1].startswith('N') or word[1].startswith('J'))]\n",
    "    #normed_tagged_words = list(set(normed_tagged_words))\n",
    "    return normed_tagged_words\n",
    "\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "class LemmaTokenizer1(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in stuff(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline attempts - Best features will be decided using Grid Search. Lets just setup a baseline for now.\n",
    "#from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#Note: add probability True to SVC classifier to be able to use predict probability function, which\n",
    "# is crucial for the the ensemble methods tried later\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\")),\n",
    "                      ('tfidf', TfidfTransformer(use_idf = True, norm='l2')),\n",
    "                      ('log', LogisticRegression(class_weight = None )),\n",
    "                      ('clf', SVC(C = 1000000.0, gamma='auto', kernel='linear', probability = True))])\n",
    "                      #('clf', LinearSVC(C=1.0, random_state=69, penalty='l2', dual=True, tol=1e-5, class_weight = None))])\n",
    "                      #('clf', OneVsOneClassifier(LinearSVC(random_state=0)))])                    \n",
    "                      #('clf', SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75550122249388751"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predictor = text_clf.fit(train_set[\"Answer\"], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = pipeline_predictor.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>260</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>17</td>\n",
       "      <td>302</td>\n",
       "      <td>65</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1    2   3   4  5  All\n",
       "True                              \n",
       "1           9    2   0   1  0   12\n",
       "2           8  260  26   1  0  295\n",
       "3           0   36  29  10  0   75\n",
       "4           0    4   6  10  1   21\n",
       "5           0    0   4   1  1    6\n",
       "All        17  302  65  23  2  409"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some ensemble classifiers (Both averaging and boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77995110024449876"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "f_clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C = 100, penalty=\"l1\", dual = False))),\n",
    "  ('classification', forest_clf)\n",
    "])\n",
    "\n",
    "forest_predictor = f_clf.fit(arr_train, train_set.KIScore)\n",
    "f_predicted = forest_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, f_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72127139364303183"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "ada_predictor = ada_clf.fit(arr_train, train_set.KIScore)\n",
    "a_predicted = ada_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, a_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79217603911980439"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0).fit(arr_train, train_set.KIScore)\n",
    "grad_predictor = grad_clf.fit(arr_train, train_set.KIScore)\n",
    "grad_predicted = grad_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, grad_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And moving on to voting classifier with hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78484107579462103"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf5 = SGDClassifier(loss='hinge', alpha=1e-5, penalty='elasticnet', n_iter=50, random_state=69)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf5', clf5), ('clf8', clf8)], voting='hard')\n",
    "\n",
    "eclf_predictor = eclf.fit(arr_train, train_set.KIScore)\n",
    "v_predicted = eclf_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, v_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SGD', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Voting classifier with soft voting\n",
    "\n",
    "Note: The MLP classifier improved accuracy for both hard and sofr voting\n",
    "\n",
    "But, I need to do a ton of cross validation for the correct parameters and classifiers for each question type. Not to mention, need to get the grid search working well for these things.\n",
    "\n",
    "TODO: Find optimal weights for the classifiers\n",
    "\n",
    "TODO: Need to do feature engineering to get better parameters. This isnt working too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_s = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], voting='soft')\n",
    "\n",
    "eclf_s_predictor = eclf_s.fit(arr_train, train_set.KIScore)\n",
    "s_predicted = eclf_s_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, s_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf6, clf8, eclf], ['Logistic Regression', 'Random Forest', \n",
    "                                                                  'naive Bayes', 'Gradient Boosting', 'SVC', \n",
    "                                                                  'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, arr_train, train_set.KIScore, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a brute force method to find the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 109.959s\n",
      "done in 112.551s\n",
      "done in 109.979s\n",
      "done in 110.940s\n",
      "done in 110.325s\n",
      "done in 111.450s\n",
      "done in 110.288s\n",
      "done in 112.476s\n",
      "done in 111.414s\n",
      "done in 109.858s\n",
      "done in 110.469s\n",
      "done in 110.105s\n",
      "done in 110.260s\n",
      "done in 112.139s\n",
      "done in 110.405s\n",
      "done in 109.968s\n",
      "done in 110.427s\n",
      "done in 110.279s\n",
      "done in 111.278s\n",
      "done in 109.543s\n",
      "done in 110.842s\n",
      "done in 110.845s\n",
      "done in 110.397s\n",
      "done in 111.184s\n",
      "done in 110.564s\n",
      "done in 110.407s\n",
      "done in 110.773s\n",
      "done in 111.640s\n",
      "done in 111.556s\n",
      "done in 110.569s\n",
      "done in 110.662s\n",
      "done in 111.503s\n",
      "done in 109.722s\n",
      "done in 109.249s\n",
      "done in 111.455s\n",
      "done in 111.114s\n",
      "done in 110.738s\n",
      "done in 110.282s\n",
      "done in 110.194s\n",
      "done in 112.136s\n",
      "done in 110.880s\n",
      "done in 109.975s\n",
      "done in 108.855s\n",
      "done in 111.330s\n",
      "done in 112.035s\n",
      "done in 110.776s\n",
      "done in 111.422s\n",
      "done in 110.167s\n",
      "done in 110.457s\n",
      "done in 109.818s\n",
      "done in 109.679s\n",
      "done in 110.885s\n",
      "done in 109.793s\n",
      "done in 109.957s\n",
      "done in 110.148s\n",
      "done in 109.162s\n",
      "done in 109.637s\n",
      "done in 111.927s\n",
      "done in 111.218s\n",
      "done in 109.040s\n",
      "done in 110.506s\n",
      "done in 110.526s\n",
      "done in 110.205s\n",
      "done in 111.533s\n",
      "done in 110.545s\n",
      "done in 109.898s\n",
      "done in 111.323s\n",
      "done in 111.993s\n",
      "done in 110.075s\n",
      "done in 111.588s\n",
      "done in 110.395s\n",
      "done in 111.739s\n",
      "done in 111.747s\n",
      "done in 110.508s\n",
      "done in 110.084s\n",
      "done in 110.625s\n",
      "done in 114.119s\n",
      "done in 111.884s\n",
      "done in 111.339s\n",
      "done in 110.358s\n",
      "done in 110.725s\n",
      "done in 110.979s\n",
      "done in 109.825s\n",
      "done in 111.332s\n",
      "done in 110.706s\n",
      "done in 111.352s\n",
      "done in 110.435s\n",
      "done in 110.104s\n",
      "done in 109.385s\n",
      "done in 110.966s\n",
      "done in 109.427s\n",
      "done in 110.904s\n",
      "done in 110.966s\n",
      "done in 111.183s\n",
      "done in 110.261s\n",
      "done in 109.354s\n",
      "done in 110.546s\n",
      "done in 109.574s\n",
      "done in 109.532s\n",
      "done in 109.702s\n",
      "done in 111.417s\n",
      "done in 110.973s\n",
      "done in 111.138s\n",
      "done in 112.092s\n",
      "done in 109.593s\n",
      "done in 111.454s\n",
      "done in 110.574s\n",
      "done in 109.524s\n",
      "done in 110.142s\n",
      "done in 109.980s\n",
      "done in 109.776s\n",
      "done in 109.926s\n",
      "done in 111.364s\n",
      "done in 109.732s\n",
      "done in 111.377s\n",
      "done in 110.417s\n",
      "done in 115.630s\n",
      "done in 111.803s\n",
      "done in 110.548s\n",
      "done in 110.903s\n",
      "done in 110.070s\n",
      "done in 110.302s\n",
      "done in 110.812s\n",
      "done in 110.751s\n",
      "done in 110.505s\n",
      "done in 110.149s\n",
      "done in 109.658s\n",
      "done in 111.067s\n",
      "done in 111.025s\n",
      "done in 111.466s\n",
      "done in 110.533s\n",
      "done in 110.073s\n",
      "done in 111.272s\n",
      "done in 109.042s\n",
      "done in 110.527s\n",
      "done in 110.390s\n",
      "done in 110.715s\n",
      "done in 110.329s\n",
      "done in 111.408s\n",
      "done in 110.731s\n",
      "done in 110.515s\n",
      "done in 110.209s\n",
      "done in 111.310s\n",
      "done in 110.466s\n",
      "done in 110.264s\n",
      "done in 110.666s\n",
      "done in 110.159s\n",
      "done in 111.799s\n",
      "done in 110.543s\n",
      "done in 111.509s\n",
      "done in 110.144s\n",
      "done in 111.241s\n",
      "done in 110.807s\n",
      "done in 110.867s\n",
      "done in 110.473s\n",
      "done in 111.343s\n",
      "done in 111.384s\n",
      "done in 110.381s\n",
      "done in 109.924s\n",
      "done in 109.464s\n",
      "done in 110.006s\n",
      "done in 110.986s\n",
      "done in 110.677s\n",
      "done in 111.201s\n",
      "done in 110.626s\n",
      "done in 110.633s\n",
      "done in 111.013s\n",
      "done in 110.612s\n",
      "done in 110.238s\n",
      "done in 109.979s\n",
      "done in 109.626s\n",
      "done in 111.084s\n",
      "done in 110.555s\n",
      "done in 110.928s\n",
      "done in 111.283s\n",
      "done in 110.108s\n",
      "done in 109.177s\n",
      "done in 110.082s\n",
      "done in 110.718s\n",
      "done in 110.660s\n",
      "done in 110.695s\n",
      "done in 110.399s\n",
      "done in 109.470s\n",
      "done in 109.401s\n",
      "done in 111.437s\n",
      "done in 109.650s\n",
      "done in 110.938s\n",
      "done in 110.433s\n",
      "done in 110.554s\n",
      "done in 110.909s\n",
      "done in 110.962s\n",
      "done in 110.376s\n",
      "done in 110.896s\n",
      "done in 110.632s\n",
      "done in 111.853s\n",
      "done in 110.254s\n",
      "done in 110.390s\n",
      "done in 110.047s\n",
      "done in 111.622s\n",
      "done in 111.047s\n",
      "done in 112.321s\n",
      "done in 110.922s\n",
      "done in 110.293s\n",
      "done in 111.010s\n",
      "done in 110.830s\n",
      "done in 110.787s\n",
      "done in 111.359s\n",
      "done in 111.704s\n",
      "done in 109.552s\n",
      "done in 111.392s\n",
      "done in 109.774s\n",
      "done in 111.392s\n",
      "done in 110.521s\n",
      "done in 109.834s\n",
      "done in 111.493s\n",
      "done in 110.637s\n",
      "done in 109.222s\n",
      "done in 110.374s\n",
      "done in 111.063s\n",
      "done in 109.696s\n",
      "done in 110.340s\n",
      "done in 110.412s\n",
      "done in 110.692s\n",
      "done in 110.503s\n",
      "done in 111.413s\n",
      "done in 110.393s\n",
      "done in 111.071s\n",
      "done in 109.707s\n",
      "done in 111.030s\n",
      "done in 110.278s\n",
      "done in 110.315s\n",
      "done in 111.329s\n",
      "done in 117.519s\n",
      "done in 129.708s\n",
      "done in 129.763s\n",
      "done in 128.827s\n",
      "done in 121.809s\n",
      "done in 124.649s\n",
      "done in 122.702s\n",
      "done in 127.145s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:41: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w6</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.813463</td>\n",
       "      <td>0.002583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.812244</td>\n",
       "      <td>0.004289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.812235</td>\n",
       "      <td>0.003588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811023</td>\n",
       "      <td>0.004849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811016</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811016</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811007</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810405</td>\n",
       "      <td>0.002975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810403</td>\n",
       "      <td>0.002743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>0.001273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809795</td>\n",
       "      <td>0.004397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809786</td>\n",
       "      <td>0.003033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809786</td>\n",
       "      <td>0.003033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809188</td>\n",
       "      <td>0.004843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809182</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809175</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809175</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808572</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808570</td>\n",
       "      <td>0.004232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808565</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807963</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807961</td>\n",
       "      <td>0.003201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807956</td>\n",
       "      <td>0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.807956</td>\n",
       "      <td>0.002728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807954</td>\n",
       "      <td>0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807954</td>\n",
       "      <td>0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807360</td>\n",
       "      <td>0.006914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.807353</td>\n",
       "      <td>0.004838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807351</td>\n",
       "      <td>0.006515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792670</td>\n",
       "      <td>0.003388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.002666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.000621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.792659</td>\n",
       "      <td>0.002671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.791449</td>\n",
       "      <td>0.003962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791447</td>\n",
       "      <td>0.008802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.790864</td>\n",
       "      <td>0.013844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.790831</td>\n",
       "      <td>0.004210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.790246</td>\n",
       "      <td>0.011019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.790226</td>\n",
       "      <td>0.004229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.789621</td>\n",
       "      <td>0.006129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.789614</td>\n",
       "      <td>0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.789589</td>\n",
       "      <td>0.004889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.789025</td>\n",
       "      <td>0.012463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.788987</td>\n",
       "      <td>0.001899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.006872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787806</td>\n",
       "      <td>0.015458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787799</td>\n",
       "      <td>0.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787779</td>\n",
       "      <td>0.007166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787156</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787143</td>\n",
       "      <td>0.004576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.785955</td>\n",
       "      <td>0.010551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.785362</td>\n",
       "      <td>0.015034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.784752</td>\n",
       "      <td>0.015586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.782306</td>\n",
       "      <td>0.014441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.781654</td>\n",
       "      <td>0.007827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.781044</td>\n",
       "      <td>0.008363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.778627</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.771879</td>\n",
       "      <td>0.004579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.763932</td>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      w1   w2   w4   w5   w6      mean       std\n",
       "217  3.0  3.0  1.0  2.0  1.0  0.813463  0.002583\n",
       "190  3.0  2.0  1.0  2.0  1.0  0.812244  0.004289\n",
       "211  3.0  2.0  3.0  3.0  1.0  0.812235  0.003588\n",
       "214  3.0  3.0  1.0  1.0  1.0  0.811023  0.004849\n",
       "145  2.0  3.0  2.0  2.0  1.0  0.811016  0.003563\n",
       "238  3.0  3.0  3.0  3.0  1.0  0.811016  0.003563\n",
       "229  3.0  3.0  2.0  3.0  1.0  0.811007  0.000566\n",
       "136  2.0  3.0  1.0  2.0  1.0  0.810405  0.002975\n",
       "157  2.0  3.0  3.0  3.0  1.0  0.810403  0.002743\n",
       "181  3.0  1.0  3.0  2.0  1.0  0.810400  0.001273\n",
       "74   1.0  3.0  3.0  2.0  1.0  0.809795  0.004397\n",
       "101  2.0  1.0  3.0  2.0  1.0  0.809786  0.003033\n",
       "172  3.0  1.0  2.0  2.0  1.0  0.809786  0.003033\n",
       "223  3.0  3.0  2.0  1.0  1.0  0.809191  0.005421\n",
       "187  3.0  2.0  1.0  1.0  1.0  0.809188  0.004843\n",
       "226  3.0  3.0  2.0  2.0  1.0  0.809182  0.002425\n",
       "199  3.0  2.0  2.0  2.0  1.0  0.809175  0.001310\n",
       "208  3.0  2.0  3.0  2.0  1.0  0.809175  0.001310\n",
       "196  3.0  2.0  2.0  1.0  1.0  0.808572  0.003199\n",
       "127  2.0  2.0  3.0  2.0  1.0  0.808570  0.004232\n",
       "184  3.0  1.0  3.0  3.0  1.0  0.808565  0.001269\n",
       "160  3.0  1.0  1.0  1.0  1.0  0.807963  0.005217\n",
       "119  2.0  2.0  2.0  2.0  1.0  0.807961  0.003201\n",
       "220  3.0  3.0  1.0  3.0  1.0  0.807956  0.001729\n",
       "239  3.0  3.0  3.0  3.0  2.0  0.807956  0.002728\n",
       "163  3.0  1.0  1.0  2.0  1.0  0.807954  0.001964\n",
       "175  3.0  1.0  2.0  3.0  1.0  0.807954  0.001964\n",
       "232  3.0  3.0  3.0  1.0  1.0  0.807360  0.006914\n",
       "230  3.0  3.0  2.0  3.0  2.0  0.807353  0.004838\n",
       "154  2.0  3.0  3.0  2.0  1.0  0.807351  0.006515\n",
       "..   ...  ...  ...  ...  ...       ...       ...\n",
       "138  2.0  3.0  1.0  2.0  3.0  0.792670  0.003388\n",
       "7    1.0  1.0  1.0  3.0  3.0  0.792659  0.002666\n",
       "85   2.0  1.0  1.0  2.0  3.0  0.792659  0.000621\n",
       "40   1.0  2.0  2.0  2.0  3.0  0.792659  0.002671\n",
       "27   1.0  2.0  1.0  1.0  2.0  0.791449  0.003962\n",
       "5    1.0  1.0  1.0  3.0  1.0  0.791447  0.008802\n",
       "180  3.0  1.0  3.0  1.0  3.0  0.790864  0.013844\n",
       "0    1.0  1.0  1.0  1.0  2.0  0.790831  0.004210\n",
       "103  2.0  1.0  3.0  2.0  3.0  0.790246  0.011019\n",
       "135  2.0  3.0  1.0  1.0  3.0  0.790226  0.004229\n",
       "25   1.0  1.0  3.0  3.0  3.0  0.789621  0.006129\n",
       "109  2.0  2.0  1.0  1.0  3.0  0.789614  0.003956\n",
       "31   1.0  2.0  1.0  2.0  3.0  0.789589  0.004889\n",
       "9    1.0  1.0  2.0  1.0  2.0  0.789025  0.012463\n",
       "58   1.0  3.0  1.0  2.0  3.0  0.788987  0.001899\n",
       "64   1.0  3.0  2.0  1.0  3.0  0.788395  0.006872\n",
       "91   2.0  1.0  2.0  1.0  3.0  0.787806  0.015458\n",
       "22   1.0  1.0  3.0  2.0  3.0  0.787799  0.011601\n",
       "37   1.0  2.0  2.0  1.0  3.0  0.787779  0.007166\n",
       "82   2.0  1.0  1.0  1.0  3.0  0.787156  0.001302\n",
       "4    1.0  1.0  1.0  2.0  3.0  0.787143  0.004576\n",
       "13   1.0  1.0  2.0  2.0  3.0  0.785955  0.010551\n",
       "46   1.0  2.0  3.0  1.0  3.0  0.785362  0.015034\n",
       "18   1.0  1.0  3.0  1.0  2.0  0.784752  0.015586\n",
       "100  2.0  1.0  3.0  1.0  3.0  0.782306  0.014441\n",
       "55   1.0  3.0  1.0  1.0  3.0  0.781654  0.007827\n",
       "28   1.0  2.0  1.0  1.0  3.0  0.781044  0.008363\n",
       "19   1.0  1.0  3.0  1.0  3.0  0.778627  0.011461\n",
       "10   1.0  1.0  2.0  1.0  3.0  0.771879  0.004579\n",
       "1    1.0  1.0  1.0  1.0  3.0  0.763932  0.006052\n",
       "\n",
       "[240 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w4', 'w5', 'w6', 'mean', 'std'))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "t0 = time()\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w4 in range(1,4):\n",
    "            for w5 in range(1,4):\n",
    "                for w6 in range(1,4):\n",
    "                        if len(set((w1,w2,w4,w5,w6))) == 1: # skip if all weights are equal\n",
    "                            continue\n",
    "                        t0 = time()\n",
    "                        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[w1, w2, w4, w5, w6], voting = 'soft')\n",
    "                        scores = cross_val_score(eclf, \n",
    "                                                 arr_train,\n",
    "                                                 train_set.KIScore,\n",
    "                                                 cv=3,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs= -1)\n",
    "                        \n",
    "                        print(\"done in %0.3fs\" % (time() - t0))\n",
    "                        df.loc[i] = [w1, w2, w4, w5, w6, scores.mean(), scores.std()]\n",
    "                        i += 1\n",
    "                        \n",
    "#print(\"done in %0.3fs\" % (time() - t0))\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77995110024449876"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators = 100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                      max_depth=1, random_state=0)\n",
    "clf6 = SVC(C = 1000000.0, gamma='auto', kernel='rbf', probability = True)\n",
    "clf8 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf_w = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), \n",
    "                                    ('clf6', clf6),\n",
    "                                   ('clf4', clf4), ('clf8', clf8)], \n",
    "                                                  weights=[3, 3, 1, 2, 1], voting = 'soft')\n",
    "\n",
    "eclf_w_predictor = eclf_w.fit(arr_train, train_set.KIScore)\n",
    "w_predicted = eclf_w_predictor.predict(arr_dev)\n",
    "accuracy_score(dev_set.KIScore, w_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! Not very good\n",
    "\n",
    "Lets see which categories we are getting wrong (Don't be 2!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-788ae8ebe01f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pd.crosstab(dev_set.KIScore, w_predicted, \n\u001b[0m\u001b[0;32m      2\u001b[0m            \u001b[0mrownames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predicted'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             margins=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "pd.crosstab(dev_set.KIScore, w_predicted, \n",
    "           rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it looks like we should try and get more data. Might not be possible without mixing up student responses.\n",
    "\n",
    "To squeeze voting classifiers into the grid, I'm restricted to using only classifiers that provide a predict_pobability function. Might be worth trying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 20000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l1', 'l2'),\n",
    "    #'clf_kernel': ('rbf', 'linear'),\n",
    "    'clf__C': (1000, 10000)\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV( pipeline, parameters, n_jobs=-1, verbose=1, cv = 9)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_set[\"Answer\"], train_set.KIScore)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_grid = grid_search.predict(dev_set[\"Answer\"])\n",
    "accuracy_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cohen_kappa_score(dev_set.KIScore, predicted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have exhausted all classifiers without really messing around with feature engineering or feature selection, lets add custom features to the pipeline using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Classifier for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78484107579462103"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting with a feature on the length of the answer\n",
    "#Every class created for the custom feature needs to have a method to transform and fit the data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Filter for question 3 to remove the David's claim condition at start\n",
    "filter_answer = lambda x : ' '.join(i for i in x.split() if not (i.startswith('david') or\n",
    "                                                                  i.startswith('claim')))\n",
    "\n",
    "#train_set['Answer'] = train_set['Answer'].apply(filter_answer)\n",
    "#dev_set['Answer'] = dev_set['Answer'].apply(filter_answer)\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text)}\n",
    "                for text in posts.tolist()]\n",
    "\n",
    "class Keywords_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'radiat' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "class Trap_Radiation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'Radiation': 'trap' in [ps.stem(i) for i in text.split()]\n",
    "                               or 'keep' in [ps.stem(i) for i in text.split()]}\n",
    "                for text in posts.tolist()]\n",
    "    \n",
    "#Required to convert a sparse matrix to a dense matrix. Vectorizers give out a sparse matrix but some \n",
    "#classifiers need a dense matrix to perform classification\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ('body_stats', Pipeline([\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_radiate', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Keywords_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('key_words_trap', Pipeline([ # Give low weight\n",
    "                    ('Radiation', Trap_Radiation()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer())  # list of dicts -> feature matrix\n",
    "        ])),\n",
    "        ('bag_of', Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(1, 3), tokenizer=LemmaTokenizer(),  \n",
    "                                              max_df=0.25, max_features= 15000, token_pattern=r'\\b\\w+\\b', \n",
    "                                              stop_words=\"english\"))\n",
    "                    #('tfidf_transformer', TfidfTransformer(use_idf = True, norm='l2'))\n",
    "        ]))\n",
    "    ],\n",
    "    # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'body_stats': 1,\n",
    "            'key_words_radiate': 0.5,\n",
    "            'key_words_trap': 1.0,\n",
    "            'bag_of': 0.5        \n",
    "        },\n",
    "    )),\n",
    "    ('to_dense', DenseTransformer()),    \n",
    "    #('dim', LinearDiscriminantAnalysis(n_components=2)),\n",
    "    #('clf', SVC(kernel='linear'))  # classifier\n",
    "    ('clf', eclf_w)  # classifier\n",
    "])\n",
    "\n",
    "\n",
    "p_predictor = pipeline.fit(train_set['Answer'], \n",
    "                                  train_set.KIScore)\n",
    "\n",
    "predicted = p_predictor.predict(dev_set['Answer'].values)\n",
    "accuracy_score(dev_set.KIScore, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 11   1   0   0   0]\n",
      " [  0 286   9   0   0]\n",
      " [  0  49  18   8   0]\n",
      " [  0   3  13   5   0]\n",
      " [  0   1   3   1   1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAAEpCAYAAAD4Vxu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x230f2a62710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(dev_set.KIScore, predicted)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Graded answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the heat of the car is other than the outside air because the heat is wrapped it is wrapped because of the type of metal the ca is\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the inflamed radiation that has entered the car will be harder to escape once the inflamed radiation is in the car it will take a while to release\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "the car has been sitting out for a week the convention makes the car warmer than outside i ve had this kind of experience once it was cold but when i got in the car it was warm the doors were open so the cold air blew in\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "even though it is cold outside the heat has been wrapped in his car\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "his car might have the greenhouse effect so his car might be other inside or other outside or the same temperature\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car got other from being in the sun for a week the hot air inside has been wrapped but not able to escape so daytime heat gets in it cannot get out again therefore the inside must be other although it is cold that day it might have been hot the rest of the week\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "i picked exactly the same as the air out side because if it has been sitting in the sun for a week it would heat up than cool down at night then when morning comes it would heat up again\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the air wrapped in the car does not escape and reach equilibrium with the air outside but instead it is warmer than the air outside because the heat is wrapped in the car\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the inside of the car will be warmer than the outside because even if it is a cold day there is still radiation of the sun that can heat up the car since it is made of metal\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "i think that is going to be warmer than the outside air because the car was marked in the sun for a week so the air got hot inside the car\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i leave that the car would be colder then the outside air because the car has windows so the solar energy would reflect on the windows and it might be warm inside the car but not as warm\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "although it is a cold day the sun s energy shines into the car and the car traps most of its energy inside thus it would be warmer inside the car than the outside\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "it is warmer because it has been absorbing sunlight and it traps the heat inside\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "if assuming all doors and windows are closed the air is contained and if it is sunny will be heated as it is not affected by the cold air outside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the inside of the car will be warmer because the car will trap the radiation entering the car radiation can enter the car but cannot escape called greenhouse effect the sun s radiation will slowly warm the car up during an any temperature day\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it s warmer in the car because the air is hot and the cars windows were closed but still air can get in the last time he drove the car the hot air got inside of the car too\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it is warmer because the heat get traced and the heat can go out and it will get other\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the temperature of the air inside the car is warmer than the outside air because all the air is wrapped inside of the car i think that it will be warmer because it is marked in the sun but it is marked in the sun for a whole week also if there is hot weather it is going to be other the cars engine heating up from use could change the temperature inside the car a little it also could be because the metal on the car gets heated by the sun which affects the temperature in the car a lot this even mostly depends on if the car is an insulated or conductor i also think it could be exactly the same as the outside air because heat goes to the colder object this means that the heat will travel to the car until the car and the outside air are the same temperature\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the solar energy from the day bar left his car was stored\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car has been sitting outside in the hot sun for a week so i think that the car has collected lots of heat energy while sitting outside in the sun\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the inside has wrapped solar radiation heat in the car for a week through the window\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "it would be warmer because of the sun s radiation from that her week\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "the car that is marked in the sun will be a bit warmer than outside because the car has not been driven for a week and it has been absorbing heat while being marked outside in a source of heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car is marked in the sun and he hasn t driven the car for a week and even though it s cold outside the sun is still heating up the car this is an example of radiation\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "even though it is cold outside the light from the sun travels through the windows and the windshield and that converts into heat since the windows and doors and probably closed much of the heat is retained keeping the inside of the car warmer than the outside air\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "radiation heat the car and condition heat the walls of the car convention circulates heat in the car\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the sunlight makes heat in the car and the heat can get out of the car\n",
      "Graded as: 2\n",
      "Actual grade is 5\n",
      "\n",
      "the temperature inside the car will be warmer than the temperature outside because the inside of the car is like an insulated and the there is sunlight being directed at the car the solar radiation will turn into heat as it enters through the glass windshield\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the car can not release he warm air\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "if the car has been in the sun the for a week without moving then it is probably other than the air around it i say this because maybe the rest of the days that the car has been there were hot and that air went inside the car and is still there so the heat is wrapped also maybe the sun being on the car on a cold day keeps the car warmer\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the car will hold the heat that it collects from the sun just like a green house would\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think it is warmer than the outside because the solar radiation can come in and become heat but the heat can not leave so more heat will come in and then eventually it will be other because there is more heat\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the temperature of the inside of bar s car is warmer because the heat has been wrapped inside from it being marked in the sun radiation enters the car but it gets wrapped like the greenhouse effect and heat stays in also with the dark car it absorb light even more\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "the greenhouse affect took place in this example this is when warm energy enters a container and it s hard for it to leave so the container heat up i think\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "convention will heat up the car from condition and the condition is caused by radiation\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "it must be colder than the outside air because all the heat is wrapped inside and turns cold because when the temperature outside drops the inside must be cold too\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "the air inside the car will be warmer than the air outside because the car was never opened so the air got warmer from the sunlight\n",
      "Graded as: 2\n",
      "Actual grade is 4\n",
      "\n",
      "because the radiation from the sun can keep the car warm\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "the sun will heat up the car to a other temperature than the temperature of the air warm air won t be able to leave because the car is a vessel again as in the pretext the question below doesn t make sense if the day is freezing would that affect the temperature more than the sun they aren t comparable\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car is warmer because the car is under direct sunlight so the car is receiving solar radiation that passes through its windows since the windows are glass it provides great induration so that the solar radiation that has been converted into heat can not escape so overcome the car will slowly heat up\n",
      "Graded as: 4\n",
      "Actual grade is 5\n",
      "\n",
      "because the sunlight passes through the glass and turn to heat and also glass is a good insulated which can trap the heat inside the car\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "the car contains heat more and if it was in the sun it would warm up\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "i think this because it s bend sitting in the sun for a week so it s abort a lot of heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because all that air that has been wrapped in the car has not been exposed to the cold air so when you open your car it should feel warmer because it s been in the sun and not missed with for awhile\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "inside the car would be warmer than the air outside because the car traps the air and takes it\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "since it has been marked outside for a week in the sun the car has wrapped heat so it will be warmer than the air outside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think that although the car has not been driven and the air is cool outside i think the air will be warmer inside from the heat of the sun\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think that it will be warmer then the air outside because the air is wrapped inside a hot car in a hot day and the sun will reflect strait down to the glass which will make the inside of the car other\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car is made of metal and glass and absorb the heat from the sky and keeps the inside of the car warmer than the outside air\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car is sealed and will take in solar radiation\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "i think it is warmer than the outside air because the car probably acts as a greenhouse which traps heat energy emitted from the sun\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it would be warmer because if it was marked outside on a hot day because the sun rays get wrapped in the car and turns into heat\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "because it would heat up because when it s cold things also tend to heat up and because the sunlight is reflected off and hitting the car\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think it will be warmer than the air outside because it has been in the sun for week and it is cold outside but the doors are shut so cold air can get in so the warm air is all alone inside the car\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because the car was outside for so long sun would shine rays into the car and the heat would get wrapped in it\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it is warmer that the outside because if you were to leave a car in the sun for a long time then the sun will shine solar radiation on the windshield and then the inside of the car will attract then heat and stay like that\n",
      "Graded as: 3\n",
      "Actual grade is 5\n",
      "\n",
      "the car has a smaller space which makes it easier to trap in heat especially if the sun is directed at the marked car\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "since the heat was wrapped inside the car it will feel other inside than the air outside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i believe it is warmer then the outside air because the outer part of the car like the crust is exposed so it is cooper like the weather the inside like the core hasn t been exposed so it should be normally warmer then you add the rays of the sun and the heat that has been stuck in the car\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "if the car has not been driven in a week and has been sitting in the sun it is going to be hot also the windows reflect light into the car making it even more hot\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the inside of the car will be other because it traps the heat in from earlier in the day and it is insulated\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "because air is wrapped inside his her car which would mean the molecule are not able to escape which heat up his car also no outside air can penetrate inside the car\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car is warmer because the heat inside the car is hot because the heat is traced inside the car while the air outside is cold\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it is warmer than the outside because it uses condition the heat comes into the car and it gets hot since there are windows that are all the way up the heat will not evacuate the vehicle\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it would be warmer because if it is left alone then the warm are would be wrapped inside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car has been marked outside for a week in the sun so the sunlight has probably gone through the windows and the heat has been wrapped in the car it is cold outside but the car has been marked in sunlight for so long the inside of the car must me warmer than the outside\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "if it is cold then the air gets in faster than light does through the small windows\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the outside would be colder because in the car the heat is confined so it would stay warmer longer but outside the air would release a lot of the heat it had before\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "sense the car has been sitting in the sun the car starts to heat up sense the metal tracts the sun the color of the car also matters because darker colors like black attract more sun but lighter colors attract less sun\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "in energy\n",
      "Graded as: 2\n",
      "Actual grade is 1\n",
      "\n",
      "i think it is warmer because the light from the sun goes onto the windshield which projects the light from the sun into the car which makes the car other than the outside air making the car other it was before\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i believe the car will be other than the air outside because the car traps heat in it does because the metal conducts heat\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n",
      "i know that it would be warmer because all the hot air is going inside for a week and none of the air is coming out and just some of the seats stay warm\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "radiation is the heat from the sun and the elctromagnetic waves will absorb through the car\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "it would be warmer than the outside because it would be heated up by being marked in the sun\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think it will be warmer than the air outside because the air will stay inside and the heat from the sun will warm up the air inside and will cause the air inside to be warmer than the air outside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i picked warmer than the outside air because if its cold outside and the car is sitting right where the sun is directed that its most likely to be warmer than the outside air\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the sun uses its radiation to hit the car this then allows condition to heat up the car inside and finally convention spreads throughout the car making it hot\n",
      "Graded as: 4\n",
      "Actual grade is 3\n",
      "\n",
      "because if you put your car that you haven t drove for a while then the guns rays well hit the windshield and it will make it warmer\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "because heat gets wrapped in the materials and kept in through the glass but at the same time when i get in the car in the morning its colder so i m confused\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it would be warmer than the outside air because if the car hasn t been driven for a week and its been in the sun the whole time the car will absorbed the heat and science there is know way the heat can get out of the car the heat will just keep building up\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the heat energy from the sun will trap itself inside the car and will not escape heating it up this is an example of radiation\n",
      "Graded as: 3\n",
      "Actual grade is 4\n",
      "\n",
      "if a car is other than it is on the outside on a hot day then it must be warmer than the outside air on a cold day because the car acts as a greenhouse and is able to trap heat\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i chose warmer than the outside air because the glass heat up and it makes the whole car other even if there wasn t a lot of sunlight\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "i think this because the car original had warmer air also it was marked directly into the sun even if its a cold day the heat from the sun would make the car warmer\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "it is other because the space is limited and the hot air will keep circulating around the car while outside the air has the whole atmosphere to rise up and cool\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the car will collect heat from the sun causing the air in the car to be warmer than the air outside because the sun gives of heat than letting the car become warmer from collecting heat letting the car become warmer than the cold air outside\n",
      "Graded as: 2\n",
      "Actual grade is 3\n",
      "\n",
      "the heat is still traced in the car but the oxygen died out\n",
      "Graded as: 3\n",
      "Actual grade is 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values = list(predicted)\n",
    "actual = dev_set.values.tolist()\n",
    "\n",
    "for (z,y) in zip(actual, predicted_values):\n",
    "    if (str(z[2]) == str(y)):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"{}\".format(z[1]))\n",
    "        print(\"Graded as: {}\".format(str(y)))\n",
    "        print(\"Actual grade is {}\".format(str(z[2])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prediction probabilities to see confidence level of predictions and defer to manual grader if needed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
